<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0218570</article-id>
<article-id pub-id-type="publisher-id">PONE-D-19-04507</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Thermodynamics</subject><subj-group><subject>Entropy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Phonetics</subject><subj-group><subject>Consonants</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Happiness</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Happiness</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Acoustic signals</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Perception of affect in unfamiliar musical chords</article-title>
<alt-title alt-title-type="running-head">Perception of affect in unfamiliar musical chords</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0250-378X</contrib-id>
<name name-style="western">
<surname>Smit</surname>
<given-names>Eline Adrianne</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4688-8004</contrib-id>
<name name-style="western">
<surname>Milne</surname>
<given-names>Andrew J.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Dean</surname>
<given-names>Roger T.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Weidemann</surname>
<given-names>Gabrielle</given-names>
</name>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>MARCS Institute for Brain, Behaviour and Development, Western Sydney University, Milperra, NSW, Australia</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>School of Social Sciences and Psychology, Western Sydney University, Milperra, NSW, Australia</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Loui</surname>
<given-names>Psyche</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Northeastern University, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">e.smit@westernsydney.edu.au</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>21</day>
<month>6</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<year>2019</year>
</pub-date>
<volume>14</volume>
<issue>6</issue>
<elocation-id>e0218570</elocation-id>
<history>
<date date-type="received">
<day>14</day>
<month>2</month>
<year>2019</year>
</date>
<date date-type="accepted">
<day>4</day>
<month>6</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Smit et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0218570"/>
<abstract>
<p>This study investigates the role of <italic>extrinsic</italic> and <italic>intrinsic</italic> predictors in the perception of affect in mostly unfamiliar musical chords from the Bohlen-Pierce microtonal tuning system. Extrinsic predictors are derived, in part, from long-term statistical regularities in music; for example, the prevalence of a chord in a corpus of music that is relevant to a participant. Conversely, intrinsic predictors make no use of long-term statistical regularities in music; for example, psychoacoustic features inherent in the music, such as roughness. Two types of affect were measured for each chord: pleasantness/unpleasantness and happiness/sadness. We modelled the data with a number of novel and well-established intrinsic predictors, namely <italic>roughness</italic>, <italic>harmonicity</italic>, <italic>spectral entropy</italic> and <italic>average pitch height</italic>; and a single extrinsic predictor, <italic>12-TET Dissimilarity</italic>, which was estimated by the chord’s smallest distance to any 12-tone equally tempered chord. <italic>Musical sophistication</italic> was modelled as a potential moderator of the above predictors. Two experiments were conducted, each using slightly different tunings of the Bohlen-Pierce musical system: a just intonation version and an equal-tempered version. It was found that, across both tunings and across both affective responses, all the tested intrinsic features and 12-TET Dissimilarity have consistent influences in the expected direction. These results contrast with much current music perception research, which tends to assume the dominance of extrinsic over intrinsic predictors. This study highlights the importance of both intrinsic characteristics of the acoustic signal itself, as well as extrinsic factors, such as 12-TET Dissimilarity, on perception of affect in music.</p>
</abstract>
<funding-group>
<funding-statement>This work is funded by the MARCS Postgraduate Scholarship, that was granted to Ms. Eline Smit for her PhD Candidature. The funder is the MARCS Institute for Brain, Behaviour and Development, Western Sydney University. Dr Andrew Milne is the recipient of an Australian Research Council Discovery Early Career Researcher Award (project number DE170100353) funded by the Australian Government. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="8"/>
<page-count count="28"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All data files are available from the Data Archiving and Networked Services (DANS) database (DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17026/dans-zz2-6urt" xlink:type="simple">10.17026/dans-zz2-6urt</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<sec id="sec002">
<title>Perception of affect in unfamiliar musical chords</title>
<p>While it is well recognised that complex musical compositions can induce strong emotional responses, it has also been found that even simple musical events such as two- or three-tone combinations (i.e. dyads or triads) can convey affect [<xref ref-type="bibr" rid="pone.0218570.ref001">1</xref>–<xref ref-type="bibr" rid="pone.0218570.ref002">2</xref>]. In this article, we examine affective responses–pleasantness and happiness–to unfamiliar microtonal triads and we identify a number of features of these chords that are associated with these affective responses. Dyads or triads are often tested according to their perceived consonance (e.g. [<xref ref-type="bibr" rid="pone.0218570.ref003">3</xref>–<xref ref-type="bibr" rid="pone.0218570.ref005">5</xref>]) or their perceived happiness [<xref ref-type="bibr" rid="pone.0218570.ref006">6</xref>–<xref ref-type="bibr" rid="pone.0218570.ref008">8</xref>]. In order to explain the origin of these affective responses, studies often focus on whether they are caused by <italic>intrinsic</italic> features such as roughness and harmonicity, or <italic>extrinsic</italic> features such as familiarity and cultural cues (e.g. [<xref ref-type="bibr" rid="pone.0218570.ref009">9</xref>–<xref ref-type="bibr" rid="pone.0218570.ref011">11</xref>]). Extrinsic features are, in part, derived from long-term statistical regularities in music or in co-occurrences of music and non-musical events. It follows from this definition that extrinsic features depend not just on the stimulus itself but also on features not in the stimulus. For instance, the prevalence of a specific chord type (e.g., major, minor, diminished, augmented) in a corpus of music is an extrinsic feature that can be used to model chord-type familiarity and the positive emotions that are typically associated with familiar experiences. Similarly, the prevalence of co-occurrences between chord types and happy or sad events (e.g., in movies [<xref ref-type="bibr" rid="pone.0218570.ref012">12</xref>]) is an extrinsic feature that can be used to model long-term associative learning. On the other hand, intrinsic features do not make use of long-term statistical regularities. For example, psychoacoustic features such as the roughness or harmonicity of a chord are not calculated from long-term statistical regularities, hence they are intrinsic [<xref ref-type="bibr" rid="pone.0218570.ref013">13</xref>].</p>
<p>The above-defined distinction between intrinsic and extrinsic features is useful for the following reasons. If an intrinsic feature is demonstrated to be typically involved in specific affective responses, this same feature–because it does not rely on long-term statistical regularities–has the capacity to induce similar affects across different musical cultures and so is a potential musical universal. By definition, this capacity cannot exist for extrinsic features because they are derived, in part, from a statistical analysis/exposure of/to a musical culture. The potential causal antecedence of intrinsic features makes them particularly interesting, as identifying them gives us insight into why music takes the forms it does and why it communicates in the way it does [<xref ref-type="bibr" rid="pone.0218570.ref014">14</xref>]. They are also interesting to a composer who wishes to create new forms or to use musical features that have the capacity for a broader cultural reach.</p>
<p>Music perception studies looking at the role of extrinsic and intrinsic predictors by observation of the profound differences in musical preferences across time and culture (e.g. [<xref ref-type="bibr" rid="pone.0218570.ref015">15</xref>–<xref ref-type="bibr" rid="pone.0218570.ref016">16</xref>]) have often tested only a limited number of intrinsic features (e.g. [<xref ref-type="bibr" rid="pone.0218570.ref017">17</xref>–<xref ref-type="bibr" rid="pone.0218570.ref018">18</xref>]). Additionally, often only Western musical stimuli are used with Western enculturated participants, who have had regular and longitudinal exposure to Western music. This makes it difficult to test for the effects of intrinsic predictors, as participants are very familiar with the statistical regularities for this particular musical culture. In order to test which and to what extent intrinsic features are having a consistent effect on affective responses, familiarity and prior exposure to stimuli should be minimized as much as possible. This can be done by either finding participants naïve to Western music, or by using unfamiliar music with statistical regularities unknown to the participants. Achieving the first is difficult, as demographic groups naïve or limited in their exposure to Western music are becoming increasingly rare. Besides, studying such populations requires costly and time-consuming fieldwork. An example of such a cross-cultural study was done by McDermott et al. [<xref ref-type="bibr" rid="pone.0218570.ref019">19</xref>] who tested the effect of several intrinsic qualities of chords on consonance and dissonance perception in an Amazonian population. Their results showed support for the hypothesis that preference for consonance over dissonance can be absent in cultures that have not had regular exposure to Western music. However, such studies would be difficult to replicate and as such to establish the reliability of the findings. Occasionally, studies use music affiliated with non-Western cultures to examine affective responses to music (e.g. [<xref ref-type="bibr" rid="pone.0218570.ref009">9</xref>]). However, these systems often have similarities with Western systems, such as the use of octaves. Also, due to a growing multicultural society and the easy spread of music through the internet, one can have access and be exposed to multiple musical cultural systems, which is hard to control for.</p>
<p>The aim of this research is to apply three methodological innovations in an alternative and systematic approach to examine affective responses to unfamiliar musical chords. First, microtonal systems have been proven to be particularly helpful in discovering underlying innate mechanisms of music perception outside of a specific cultural context [<xref ref-type="bibr" rid="pone.0218570.ref020">20</xref>–<xref ref-type="bibr" rid="pone.0218570.ref024">24</xref>, <xref ref-type="bibr" rid="pone.0218570.ref014">14</xref>]. Second, non-microtonal studies have often considered only a rather limited number of distinct features (such as only roughness and/or harmonicity). This paper combines multiple novel and established extrinsic and intrinsic predictors to model affective responses in a systematic approach. Third, both pleasantness and happiness responses will be tested with the same stimuli and participants allowing them to be compared.</p>
<p>To this end, this study assessed in two experiments the affective evaluation of unfamiliar musical chords outside the familiar Western musical context, using triads from the Bohlen-Pierce (BP) system, in order to examine the impact of intrinsic features, as well as the effect of each chord’s dissimilarity to the culturally prevalent 12-tone equal temperament (12-TET) tuning system. The Bohlen-Pierce system does not use the octave as a repetition interval and it contains many intervals that are distinct to those found in conventional Western music. Affective evaluation was measured as ratings of consonance (pleasant/unpleasant) and valence (happy/sad). To ensure that differences in chord ratings were not due to a general misconception of the term consonance by non-musicians, we measured ratings of perceived pleasantness and the term consonance was not mentioned to participants. Participants were clearly instructed to indicate what the chord expressed (perceived emotion), rather than how the chord made them feel (felt emotion).</p>
<p>The intrinsic features used in this study are mathematically modelled and are a combination of novel and well-established predictors, namely roughness, harmonicity, spectral entropy and average pitch height. We modelled one single extrinsic feature, 12-TET Dissimilarity, which is estimated as the sum of differences between the tones of each BP chord and the tones in the nearest twelve-tone equal temperament (12-TET) chord. Additionally, the musical sophistication of each participant was measured to establish whether it moderated the impact of any of the intrinsic or extrinsic features. This set of intrinsic and extrinsic features provides the basis for a systematic approach in a correlative study of music and affect by using a musical system that has been understudied in music perception research.</p>
</sec>
<sec id="sec003">
<title>The Bohlen-Pierce system</title>
<p>The Bohlen-Pierce system was independently conceived by Heinz Bohlen, Kees van Prooijen and John Robinson Pierce in the 1970s and 1980s [<xref ref-type="bibr" rid="pone.0218570.ref025">25</xref>]. Bohlen-Pierce (BP) is a tuning system whose intervals contain approximations only of odd integer frequency ratios, instead of a combination of odd and even as found in the Western diatonic-chromatic system. The interval of repetition is a <italic>tritave</italic> of 3:1, which is analogous to the octave of 2:1 in the aforementioned Western systems (there are no 2:1-like intervals in BP). The tritave is typically divided into 13 steps and has many intervals that are close to low-integer ratios. Intervals considered consonant in Western music often closely approximate low-integer ratios (and the converse). In the BP system, it is typically asserted, from a theoretical point of view, that a similar principle will hold. A more detailed description of the scale and its chords can be found in Mathews et al. [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>].</p>
<p>From the chromatic BP scale, 78 unique triads can be formed when including the tritave. This number decreases to 66 triads when the <italic>tritave</italic> is excluded. These 66 chords form the stimulus corpus for the current experiment. For experiment 1, the just intonation version of the scale was used, as described by Bohlen [<xref ref-type="bibr" rid="pone.0218570.ref027">27</xref>] in his original paper. Due to slight tuning differences in just intonation, the number of chords tested raises therefore from 66 to 414. The second experiment uses the equal-tempered version of the scale similar to Mathews et al. [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>] and tested the 66 chords. Just intonation is a pure tuning using intervals that are based on small integer ratios, which makes the system sound very consonant. These pure intervals do not work for every key, which makes it impractical for some instruments. In equal temperament, intervals do not consist of whole number ratios but are changed slightly. In this way, an octave is divided into twelve equally-sized intervals, making it a very practical tuning that can be used by most instruments.</p>
<p>The cumulative cents positions per “semitone” (i.e. smallest) step used for both experiments are presented in <xref ref-type="table" rid="pone.0218570.t001">Table 1</xref>. The starting tone is represented as 0 cents.</p>
<table-wrap id="pone.0218570.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.t001</object-id>
<label>Table 1</label> <caption><title>Cumulative cents positions of the equal-tempered and the just intonation Bohlen-Pierce scales.</title></caption>
<alternatives>
<graphic id="pone.0218570.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Semitone step</th>
<th align="left">Interval name</th>
<th align="left">Equal temperament (cents)</th>
<th align="left">Just intonation (cents)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">1</td>
<td align="left">BP first</td>
<td align="left">146.3</td>
<td align="left">133.2</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">BP second</td>
<td align="left">292.6</td>
<td align="left">301.8</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">BP third</td>
<td align="left">438.9</td>
<td align="left">435.1</td>
</tr>
<tr>
<td align="left">4</td>
<td align="left">BP fourth</td>
<td align="left">585.2</td>
<td align="left">582.5</td>
</tr>
<tr>
<td align="left">5</td>
<td align="left">BP fifth</td>
<td align="left">731.5</td>
<td align="left">736.9</td>
</tr>
<tr>
<td align="left">6</td>
<td align="left">BP sixth</td>
<td align="left">877.8</td>
<td align="left">884.4</td>
</tr>
<tr>
<td align="left">7</td>
<td align="left">BP seventh</td>
<td align="left">1024.1</td>
<td align="left">1017.6</td>
</tr>
<tr>
<td align="left">8</td>
<td align="left">BP eighth</td>
<td align="left">1170.4</td>
<td align="left">1165.0</td>
</tr>
<tr>
<td align="left">9</td>
<td align="left">BP ninth</td>
<td align="left">1316.7</td>
<td align="left">1319.4</td>
</tr>
<tr>
<td align="left">10</td>
<td align="left">BP tenth</td>
<td align="left">1463.0</td>
<td align="left">1466.9</td>
</tr>
<tr>
<td align="left">11</td>
<td align="left">BP eleventh</td>
<td align="left">1609.3</td>
<td align="left">1600.1</td>
</tr>
<tr>
<td align="left">12</td>
<td align="left">BP twelfth</td>
<td align="left">1755.6</td>
<td align="left">1768.7</td>
</tr>
<tr>
<td align="left">13</td>
<td align="left">Tritave</td>
<td align="left">1901.9</td>
<td align="left">1901.9</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>From a musical perspective, equally tempered scales are often considered advantageous because of their practicality. As the scale is divided into a certain number of equal steps, it is possible to freely transpose up and down and move between keys, which is advantageous for composers and performers. An argument for using just intonation is that the intervals more precisely correspond to the previously mentioned low-integer ratios, which are theoretically highly consonant. In the case of the Bohlen-Pierce system, the deviation in cents between steps of the equal-tempered and the just intonation BP scale is generally smaller than the deviation between the equal-tempered and just intonation Western equivalents [<xref ref-type="bibr" rid="pone.0218570.ref025">25</xref>] and it would therefore not be surprising if there is not much difference in perception of the two tuning systems. However, the just intonation BP scale was originally described by Bohlen [<xref ref-type="bibr" rid="pone.0218570.ref027">27</xref>] whereas the equal tempered version is most often used for compositions and/or perceptual experiments. In this study, both versions of the scale are experimentally tested.</p>
<p>An interesting characteristic of BP is that some of its triadic structures are analogous to those of Western systems, for example the formation of major chords. The BP major chord (0 6 10 in BP semitone steps) is derived from the harmonic series of a pitch, similar to how a Western major diatonic chord is formed. As the pitches of a major chord approximate quite well integer multiples of the fundamental frequency, the chord is expected to be one of the most consonant chords in the scale.</p>
<p>Only a limited number of studies have assessed consonance of BP chords. Mathews et al. [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>] examined the perceived consonance of the 78 possible triads that can be formed within the range of 1 <italic>tritave</italic> of the chromatic equal-tempered version of the BP scale. Participants rated dissonance on a 7-point Likert scale with 7 being “very consonant” and 1 “very dissonant”. The concept of consonance was not specifically explained to the non-musicians. Results from their first experiment showed a large difference in ratings between the chords that were perceived as most consonant and those perceived as most dissonant (see <xref ref-type="table" rid="pone.0218570.t002">Table 2</xref>). A strong Pearson correlation (<italic>r</italic> = .68) was found between the mean dissonance of each of chord as rated by the musicians and non-musicians. Notably, the BP major chord (0 6 10) was not rated as one of the most consonant chords for the musician group, and only the 8<sup>th</sup> highest rated for the non-musician group.</p>
<table-wrap id="pone.0218570.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.t002</object-id>
<label>Table 2</label> <caption><title>Consonance ratings for the most consonant and dissonant rated BP chords in equal temperament (Mathews et al., 1988).</title> <p>Means were taken over 72 responses per chord.</p></caption>
<alternatives>
<graphic id="pone.0218570.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" colspan="4">Most consonant</th>
<th align="center" colspan="4">Most dissonant</th>
</tr>
<tr>
<th align="center" colspan="2">Musicians</th>
<th align="center" colspan="2">Untrained</th>
<th align="center" colspan="2">Musicians</th>
<th align="center" colspan="2">Untrained</th>
</tr>
<tr>
<th align="left">Chord</th>
<th align="left">Mean</th>
<th align="left">Chord</th>
<th align="left">Mean</th>
<th align="left">Chord</th>
<th align="left">Mean</th>
<th align="left">Chord</th>
<th align="left">Mean</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">0,11,13</td>
<td align="left">5.31</td>
<td align="left">0,7,10</td>
<td align="left">4.97</td>
<td align="left">0,1,2</td>
<td align="left">1.61</td>
<td align="left">0,1,2</td>
<td align="left">2.47</td>
</tr>
<tr>
<td align="left">0,2,5</td>
<td align="left">5.25</td>
<td align="left">0,2,11</td>
<td align="left">4.494</td>
<td align="left">0,11,12</td>
<td align="left">1.67</td>
<td align="left">0,1,3</td>
<td align="left">2.64</td>
</tr>
<tr>
<td align="left">0,5,11</td>
<td align="left">5.22</td>
<td align="left">0,7,13</td>
<td align="left">4.89</td>
<td align="left">0,12,13</td>
<td align="left">1.89</td>
<td align="left">0,12,3</td>
<td align="left">2.75</td>
</tr>
<tr>
<td align="left">0,6,8</td>
<td align="left">5.17</td>
<td align="left">0,6,13</td>
<td align="left">4.83</td>
<td align="left">0,9,10</td>
<td align="left">1.89</td>
<td align="left">0,2,3</td>
<td align="left">2.81</td>
</tr>
<tr>
<td align="left">0,2,8</td>
<td align="left">5.14</td>
<td align="left">0,8,11</td>
<td align="left">4.81</td>
<td align="left">0,10,11</td>
<td align="left">1.94</td>
<td align="left">0,9,10</td>
<td align="left">3.06</td>
</tr>
<tr>
<td align="left">0,3,5</td>
<td align="left">5.03</td>
<td align="left">0,4,7</td>
<td align="left">4.78</td>
<td align="left">0,8,9</td>
<td align="left">2.00</td>
<td align="left">0,4,5</td>
<td align="left">3.06</td>
</tr>
<tr>
<td align="left">0,3,13</td>
<td align="left">5.11</td>
<td align="left">0,7,9</td>
<td align="left">4.78</td>
<td align="left">0,1,9</td>
<td align="left">2.06</td>
<td align="left">0,8,9</td>
<td align="left">3.11</td>
</tr>
<tr>
<td align="left">0,7,11</td>
<td align="left">5.08</td>
<td align="left">0,6,10</td>
<td align="left">4.75</td>
<td align="left">0,1,13</td>
<td align="left">2.22</td>
<td align="left">0,11,12</td>
<td align="left">3.11</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Mathews et al. [<xref ref-type="bibr" rid="pone.0218570.ref022">22</xref>] suggested that the high average dissonance ratings were explained by the presence of one-step intervals, but other possible intrinsic predictors were not further investigated. Interestingly, it appears that there was an influence of musical experience in the musician group. Out of the eight chords rated as most consonant, the first five contain intervals that have a close relation to diatonic intervals. Familiarity with diatonic intervals appeared to affect the ratings of those BP chords. The current experiment is based on that initial study by Mathews et al. [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>] but intends to expand on their results by, first of all, testing both the just intonation and the equal-tempered scale in the same experimental paradigm. Second, apart from consonance, valence will be tested in this study as well. Third, by also testing multiple intrinsic predictors and one extrinsic predictor, it is hoped that a more systematic explanation of affective responses will be provided. This is mostly missing in Mathews et al. [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>], as they only mention musical sophistication and the presence of one-step intervals as possible explanations for affective ratings.</p>
<p>In a recent study, Friedman et al. [<xref ref-type="bibr" rid="pone.0218570.ref028">28</xref>] used isochronous and semi-random tone sequences from the BP system to test the theory that average pitch height is correlated with happiness/sadness. Similar to the aim of the current experiment, by using tone sequences from an unfamiliar musical system, the effect of average pitch height on perceived valence could be assessed independently from previous exposure to a well-known musical system. As well as finding an association between pitch height and valence, Friedman et al. [<xref ref-type="bibr" rid="pone.0218570.ref028">28</xref>] also tested whether average pitch height had a stronger effect than average interval size, as melodies containing larger intervals have been found to be associated with happy music and melodies containing small intervals with sad music. In their experiment, 114 participants were asked to rate happiness/sadness of a set of the BP tone sequences in two different conditions (Congruent Pitch Height/Interval Size and Incongruent Pitch Height/Interval Size). A main effect for pitch height was found, showing that melodies with higher average pitch were rated slightly happier than the average. No effect of average interval size was found, which might imply that the tuning differences in just intonation and equal temperament in the current study will be negligible as well. Their results partially support prior theories [<xref ref-type="bibr" rid="pone.0218570.ref029">29</xref>–<xref ref-type="bibr" rid="pone.0218570.ref031">31</xref>] that higher average pitch height correlates with higher perceived ratings of happiness.</p>
<p>Even though the stimuli used in this experiment will be unfamiliar to the participants, at the end of the experiment they will have received short-term exposure to the Bohlen-Pierce system and might have gained some familiarity with the system [<xref ref-type="bibr" rid="pone.0218570.ref032">32</xref>]. Statistical learning might therefore occur leading to rapidly formed preferences within the unfamiliar musical system. Support for such rapid acquisition of preferences within BP has been shown by Loui [<xref ref-type="bibr" rid="pone.0218570.ref033">33</xref>]. This newly acquired exposure to the system might have an effect on their affective ratings [<xref ref-type="bibr" rid="pone.0218570.ref033">33</xref>–<xref ref-type="bibr" rid="pone.0218570.ref034">34</xref>]. This short-term learning is not the focus of our study, but an exploratory regression (detailed in the Results section) was conducted in order to estimate the effects of exposure throughout the experiment.</p>
</sec>
<sec id="sec004">
<title>Intrinsic predictors</title>
<p>As mentioned in the introduction, affective responses to music, such as pleasantness or happiness, can be modelled by intrinsic psychoacoustic predictors. These features suggest which aspects of the acoustic signal might correlate with affective responses. Crucially, such intrinsic features would not depend on prior knowledge of statistical features in the music under examination, hence might have the capacity to be independent of a listener’s musical culture. As detailed below, this study uses several intrinsic features, some of which are novel (or calculated in a novel way) and some of which have previously been used as predictors of consonance and valence in Western tonal systems. It is expected that multiple independent predictors–intrinsic and extrinsic–are simultaneously at play, rather than just one.</p>
<sec id="sec005">
<title>Roughness</title>
<p><italic>Roughness</italic> is a well-known phenomenon, first proposed by Helmholtz [<xref ref-type="bibr" rid="pone.0218570.ref035">35</xref>] as an explanation for why some musical intervals are considered dissonant or unpleasant. Roughness occurs when frequency components in the acoustic signal differ only slightly, causing interference within the same critical bandwidth of the cochlear and hence resulting in audible beating (e.g. [<xref ref-type="bibr" rid="pone.0218570.ref003">3</xref>,<xref ref-type="bibr" rid="pone.0218570.ref001">1</xref>,<xref ref-type="bibr" rid="pone.0218570.ref036">36</xref>]). Helmholtz [<xref ref-type="bibr" rid="pone.0218570.ref035">35</xref>] suggested that roughness is an explanation for the experience of dissonance, which has been the most prominent view during the 20th century [<xref ref-type="bibr" rid="pone.0218570.ref037">37</xref>]. This theory of roughness has been challenged by McLachlan et al. [<xref ref-type="bibr" rid="pone.0218570.ref038">38</xref>]. In two experiments, it was found that an increasing number of harmonics in a chord does not increase perception of dissonance, and that perception of dissonance in chords can be altered by training. Their findings, which are highly suggestive of a familiarity and music training effect on dissonance, led to a new model of dissonance based on familiarity and musical experience. The dual-process theory of dissonance proposed by Johnson-Laird et al. [<xref ref-type="bibr" rid="pone.0218570.ref039">39</xref>] assumes a combination of ‘sensory’ and ‘tonal’ dissonance, in which the first is caused by roughness and the latter by top-down cognitive processes, i.e. learned processes of tonality. This theory is based on Western tonal music, but the authors claim it should hold for unfamiliar tuning systems as well.</p>
<p>In the present study, all else being equal, negative correlations of roughness with both pleasantness and happiness ratings are hypothesized. Roughness was calculated for harmonic complex tones with 64 harmonics of relative magnitude 1<italic>/n</italic>, where <italic>n</italic> is the harmonic number. This spectrum provides a reasonable approximation of the spectrum of the piano sound used in the experiment. As the purpose of this study is whether roughness can contribute to perception in a general sense, it is not measured on the acoustic signal itself, but calculated on the theoretical properties of the generated sound.</p>
</sec>
<sec id="sec006">
<title>Harmonicity</title>
<p><italic>Harmonicity</italic> is generally described as the degree of similarity between the spectrum of a tone and a template harmonic complex tone, which consists of numerous frequencies all at integer multiples of a single fundamental frequency [<xref ref-type="bibr" rid="pone.0218570.ref040">40</xref>,<xref ref-type="bibr" rid="pone.0218570.ref024">24</xref>,<xref ref-type="bibr" rid="pone.0218570.ref041">41</xref>]. There are a variety of precise mathematical quantifications of harmonicity (see [<xref ref-type="bibr" rid="pone.0218570.ref042">42</xref>] for an overview of harmonicity models). Harmonicity is thought to contribute to pleasantness independently from roughness–consonance may therefore be not due only to a lack of unpleasant beating, but also to high harmonicity [<xref ref-type="bibr" rid="pone.0218570.ref043">43</xref>]. In this paper, we use a completely novel mathematical quantification of harmonicity, as outlined below.</p>
<p>Similar to roughness, the study’s interest in the contribution of harmonicity to perception in a general way does not require it to be measured from the acoustic signal itself. The harmonicity of a triad is here quantified by the value–at the appropriate location–of the nonperiodic relative triad expectation tensor (a matrix) [<xref ref-type="bibr" rid="pone.0218570.ref044">44</xref>] of a single harmonic complex tone with harmonics taking amplitudes of 1/<italic>n</italic>, where <italic>n</italic> is the harmonic number. The expectation tensors approximate the actual acoustic signal, but do not consider the additional difficulties of pitch perception such as frequency and amplitude masking [<xref ref-type="bibr" rid="pone.0218570.ref045">45</xref>].</p>
<p>The value of the expectation tensor provides a “count” of the number of times a given triad occurs within the harmonic series but, crucially, this count is weighted by the numbers of the harmonics involved in the chord (chords comprising high-numbered harmonics have a lower weight than lower-numbered harmonics); furthermore, perceptual inaccuracies of pitch perception are explicitly modelled to allow for a chord that is similar–but not identical–to one found in the harmonic series to gain an associated count that is penalized by distance. Unpacking these concepts is best achieved by working through a simple example.</p>
<p>Let us consider a chord with four musical pitches (in cents) arranged in a vector in ascending order (0, 400, 700, 1100); hence this is a 12-TET major seventh chord (e.g., C-E-G-B). For example, we also have an associated vector of weights (1, 0.5, 0.5, 1) for those musical pitches. Now imagine the expectation matrix as a discrete two-dimensional graph (a surface) where the horizontal and vertical axes are cents above any given <italic>reference pitch</italic>. We use the following procedure to define each value in this matrix (height of each point of the surface). Let us first choose the first pitch in the above pitch vector as the reference. With respect to that reference, there are three ordered pairs of intervals that define a triad of pitches: (400, 700), (400, 1100), (700, 1100). Each of these pairs has an associated weight which is the product of the weights of their entries; respectively: 0.25, 0.5, and 0.5. So, in the graph at entry (400, 700), we place a value of 0.25; at (400, 1100), we place a value of 0.5; at (700, 1100), we place a value of 0.5; and so on. Now we take the next pitch in the set as the reference, so we start from 400 to get the ordered pair (700–400, 1100–400) = (300, 700). This has a weight of 0.25. So, in the graph at entry (300, 700), we place a value of 0.25.</p>
<p>If the same ordered pair appears more than once in the original pitch vector, the weights are simply added in the expectation matrix. The values in the expectation matrix can, therefore, be thought of as weighted counts of triads in the original vector of pitches. Next, we convolve these “spikes” with a two-dimensional Gaussian kernel to smooth them as a model for inaccuracies or uncertainties of pitch perception. To allow us to calculate harmonicity, we use as our initial pitch vector a large number of harmonics (in these calculations, harmonics 1 to 64, in cents) and weight each harmonic by 1/<italic>n</italic>, where <italic>n</italic> is the harmonic number. From this we construct the expectation tensor, as above. This gives us a “weighted count” of triads within the harmonic spectrum, smoothed to take account of perceptual uncertainty. From this matrix, we can read off the harmonicity of any triad up to one cent resolution. If a given location in the graph (i.e., a given triad) has a high value this means that it occurs frequently in the harmonic series and/or it occurs between low-numbered (hence highly weighted) harmonics.</p>
<p>We therefore anticipate that, all else being equal, harmonicity will be positively correlated with both pleasantness and happiness.</p>
</sec>
<sec id="sec007">
<title>Spectral entropy</title>
<p><italic>Spectral entropy</italic> is a measure conventionally used to quantify the amount of noise in a signal [<xref ref-type="bibr" rid="pone.0218570.ref045">45</xref>–<xref ref-type="bibr" rid="pone.0218570.ref047">47</xref>]. Entropy is usually defined as a measure of disorder or chaos in which a high entropy represents a greater level of disorder and has applications in different fields, such as thermodynamics, information theory and statistics. In digital signal processing, spectral entropy is traditionally calculated from the power spectrum of a signal (although the magnitude spectrum can also be used). This spectrum is then scaled so that its sum is one, hence turning it into a probability mass function, here denoted <italic>x</italic>. The entropy, <italic>H</italic>(<italic>x</italic>), of this probability mass function is then calculated accordingly.</p>
<disp-formula id="pone.0218570.e001">
<alternatives>
<graphic id="pone.0218570.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0218570.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
<p>Spectral entropy is often used as a timbral descriptor of audio signals (e.g. [<xref ref-type="bibr" rid="pone.0218570.ref048">48</xref>]) but was first suggested as an intrinsic measure for modelling consonance or the “overall tonal affinity of a single pitch class set” in Milne et al. [<xref ref-type="bibr" rid="pone.0218570.ref049">49</xref>]. In the spectral entropy calculation used in this article, the complete sound spectrum of all tones in a pitch set, such as a BP chord, is smoothed with a Gaussian kernel to simulate inaccuracies of pitch perception. The resulting smoothed spectrum is then scaled, as described above, resulting in a probability mass function based on magnitudes that, through the smoothing, also incorporates uncertainty of pitch perception. The entropy of this distribution is then determined.</p>
<p>Spectral entropy, calculated like this, is a way of aggregating the spectral pitch (class) similarities [<xref ref-type="bibr" rid="pone.0218570.ref024">24</xref>,<xref ref-type="bibr" rid="pone.0218570.ref050">50</xref>,<xref ref-type="bibr" rid="pone.0218570.ref014">14</xref>,<xref ref-type="bibr" rid="pone.0218570.ref042">42</xref>,<xref ref-type="bibr" rid="pone.0218570.ref044">44</xref>] all pairs of sounds in a pitch (class) set, such as a chord or scale. This is because the greater the degree of overlap of the partials in two different sounds, the higher the spectral pitch similarity of the two sounds and the lower the overall entropy of the resulting spectrum. While spectral pitch similarity can be applied only to exactly two different spectra, spectral entropy is applied to a single spectrum–the spectrum of the composite sound. To date, spectral entropy has been used to predict the tonal affinity of a variety of microtonal scales [<xref ref-type="bibr" rid="pone.0218570.ref049">49</xref>] but, until this study, no experimental testing has been undertaken to validate these predictions. Similar to roughness, the spectrum of each tone was idealized as a harmonic complex tone with 64 harmonics with magnitudes 1<italic>/n</italic>.</p>
<p>In short, spectral entropy is a measure of the unpredictability or disorder of the spectrum produced by the chord and is expected to have a negative effect on ratings of consonance and valence. In other words, the higher the spectral entropy, the more unpredictable the chord, which is expected to result in a lower rating.</p>
</sec>
<sec id="sec008">
<title>Average pitch height</title>
<p>In this study, average pitch height is defined as the mean of the musical pitches in a given chord. So, if a chord <italic>x</italic> has musical pitches (60, 68.78, 74.63 in MIDI pitch) (this is a BP “major” chord whose root is at middle C), its average pitch height is 67.8. Average Pitch Height square will also be included in the models to allow for possible linear, “U”, or inverted “U”-shaped effects of pitch height. This is desirable because we are interested in assessing the effects on ratings of the different chord types independently from their overall pitch. A few experimental studies have tested the effects of average pitch height on perception of happiness/sadness and their results consistently show a positive relationship between pitch height and valence; where higher pitch accords with happy and lower with sad [<xref ref-type="bibr" rid="pone.0218570.ref028">28</xref>,<xref ref-type="bibr" rid="pone.0218570.ref051">51</xref>–<xref ref-type="bibr" rid="pone.0218570.ref052">52</xref>,<xref ref-type="bibr" rid="pone.0218570.ref031">31</xref>]. The association between preference and pitch height is not as robust in the general music perception literature. Several studies have found inverse relationships or an <italic>inverted-U- shaped</italic> function between preference and pitch height (see [<xref ref-type="bibr" rid="pone.0218570.ref053">53</xref>] for an overview of those studies), but it is important to note that the range of frequencies used is often larger in the studies reporting an inverse relationship or an inverted-U-shape. In the previously discussed study by Friedman et al. [<xref ref-type="bibr" rid="pone.0218570.ref028">28</xref>], the positive relation between average pitch and happiness ratings was found despite the unconventional nature of the scale and was independent of interval sizes, which is suggestive of it being a robust effect.</p>
</sec>
</sec>
<sec id="sec009">
<title>Extrinsic predictor</title>
<sec id="sec010">
<title>12-TET Dissimilarity</title>
<p>Apart from intrinsic characteristics of the sounds actually being listened to, extrinsic factors such as familiarity with a musical system play a part in music perception. As mentioned earlier, these factors are derived from long-term statistical regularities in musical events, but are not necessarily independent of stimulus features. Familiarity plays a significant role in the determination of consonance and dissonance of chords, evidenced by a decrease in dissonance perception with musical experience [<xref ref-type="bibr" rid="pone.0218570.ref054">54</xref>,<xref ref-type="bibr" rid="pone.0218570.ref043">43</xref>,<xref ref-type="bibr" rid="pone.0218570.ref055">55</xref>]. Some intervals in the BP system are quite close to Western intervals, which might positively influence ratings of their consonance and valence [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>]. We assume that 12-TET (12-tone equal temperament) is the tuning system participants will be most familiar with, as this and very similar tunings are ubiquitous in Western music. As familiarity cannot be objectively and directly measured, a predictor measuring the dissimilarity between BP chords and Western 12-test chords was included which we named <italic>12-TET Dissimilarity</italic>. Dissimilarity for a given BP chord is therefore calculated as the sum of differences, in semitones (in MIDI pitch) between it and the chord comprising only 12-TET intervals that minimizes this sum of differences. For example, given the BP chord (60, 68.78, 74.63), the 12-TET chord (59.78, 68.78, 74.78) minimizes the sum of pitch distances, hence the former chord’s <italic>12-TET Dissimilarity</italic> is |60−59.78|+|68.78−68.78|+|74.63−74.78| = 0.37. A negative effect is hypothesized for 12-TET Dissimilarity, as lower pleasantness and happiness ratings are expected for chords containing intervals that are more dissimilar from 12-TET intervals.</p>
</sec>
</sec>
<sec id="sec011">
<title>Moderator</title>
<sec id="sec012">
<title>Musical sophistication</title>
<p><italic>Musical sophistication</italic>, as defined by Müllensiefen et al. [<xref ref-type="bibr" rid="pone.0218570.ref056">56</xref>] is “a psychometric construct that can refer to musical skills, expertise, achievements and related behaviours across a range of facets that are measured on different subscales” (p. 2). This definition focuses on the multi-dimensionality of musical behaviour and includes listening skills, music-production abilities and responses to musical situations. High levels of musical sophistication are found for individuals who show more prevalent and varied musical behaviours which are executed with greater ease [<xref ref-type="bibr" rid="pone.0218570.ref056">56</xref>].</p>
<p>Musical sophistication is expected to moderate the effects of the described intrinsic and extrinsic predictors. Music perception studies often have a clear separation between two participant groups: musicians and non-musicians. However, it is difficult to pinpoint what exactly defines a musician as there is no clear consensus on the requirements of a musician [<xref ref-type="bibr" rid="pone.0218570.ref057">57</xref>]. This experiment will not separate the analysis between two groups (musicians and non-musicians) but rather employ musical sophistication as a continuum and it will therefore be treated as a continuous covariate. Musical sophistication is calculated through the psychometric tool called the Goldsmiths Musical Sophistication Index (GMSI) developed by Müllensiefen et al. [<xref ref-type="bibr" rid="pone.0218570.ref056">56</xref>]. We center the resulting musical sophistication index with respect to the population average of 81.58 and subsequently standardized. The number 81.58 originates from the initial study 147,633 of participants [<xref ref-type="bibr" rid="pone.0218570.ref054">54</xref>]. Musical sophistication is expected to modify the other effects.</p>
<p>To quickly summarize, roughness, spectral entropy and 12-TET Dissimilarity are hypothesized to have a negative effect on affective ratings, whereas harmonicity and average pitch height are expected to positively affect ratings. It is expected that musical sophistication moderates the other effects. Besides, it is expected that the slight tuning differences between just intonation and equal temperament will not have an impact on the ratings of the chords.</p>
</sec>
</sec>
<sec id="sec013">
<title>Experiments</title>
<p>In two experiments, the influence of intrinsic and extrinsic features on the perception of consonance and valence of BP chords was tested. Based on previous results by Mathews et al. [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>], it is expected that ratings of consonance and valence will be distinctly different for several chords. It is also expected that ratings of chords will not distinctively differ between just intonation and equal temperament.</p>
</sec>
</sec>
<sec id="sec014" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec015">
<title>Experiment 1</title>
<sec id="sec016">
<title>Participants</title>
<p>Sixty participants took part in the study (mean age = 25.7 years, SD = 10.55, 45 females). Participants included 46 undergraduate psychology students from Western Sydney University and the remaining 14 were recruited through word-of-mouth and social media advertisements. The participant group identified as culturally diverse (28 Australians, 6 Europeans, 14 Asians, 7 Middle Eastern, 1 Hispanic, 2 Aboriginals, 1 Native American and 1 American). Participation was respectively rewarded with course credit or $20. Thirty-four participants reported being able to play at least one musical instrument and 20 of those played for more than 5 years. Written informed consent was obtained from all participants prior to the start of the experiment, and the study was approved by the Western Sydney University Human Research Ethics Committee (H12632).</p>
</sec>
<sec id="sec017" sec-type="materials|methods">
<title>Materials</title>
<p>Stimuli were generated and displayed in MAX/MSP 7 (Cycling ‘74) in conjunction with Pianoteq 5 (v 5.8.1/20161125). The timbre of the tones was produced by Pianoteq 5 Pro (a high-quality physical synthesis model) with the instrument pack Grand D4 Daily Practice (Piano). Each trial presented consisted of a neutral musical context and a target triad. The context consisted of all thirteen individual notes of the chromatic BP scale in just intonation played twice and always in random order isochronously, immediate repetitions were not allowed, with an inter-onset interval (IOI) of 120 ms and at a MIDI key velocity of 64. The context was used to avoid effects of the previous trial on the next. The target triad followed 1000 ms after the context with a duration of 2000 ms and with a uniform MIDI key velocity value of 64. Triads were randomly selected from a subset of the 414 unique triads in BP just intonation. The chords were randomly transposed up in MIDI pitch until the 12th step of the BP scale. The lowest and the highest possible bass notes were 52 in MIDI pitch with a frequency of 164 Hz (no transposition) and 64 in MIDI pitch with a frequency of 329 Hz (highest transposition).</p>
</sec>
<sec id="sec018">
<title>Procedure</title>
<p>The experiment was conducted in a soundproof room and participants were seated behind a laptop and listened through headphones. Headphones were either Sennheiser HD 280 Pro, Beyerdynamic DT 770 Pro or Sennheiser HD 650 depending on the availability at the time of the experiment. Volume was set at a comfortable level. First, the experimental task was explained. The experiment consisted of 4 blocks of 66 trials, so in total 264 trials. In two of the four blocks, participants were asked to rate the valence of the chord, i.e. how happy or sad the chord sounded on a continuous scale (presented as a horizontal slider) containing 700 discrete values invisible to the participant. The slider (“valence”) was labelled with the words “very unhappy” on the outer left side and “very happy” on the outer right side of the slider with the central point marked with a dash. In the two other blocks, participants rated the pleasantness (i.e. “consonance”) of the chord in a similar fashion. During those blocks, the labels of the slider were “very unpleasant” and “very pleasant”, with the central point marked with a dash. The order of the blocks was randomized. Small breaks in between the blocks were allowed and encouraged, but not required. Prior to the blocks, 5 practice trials were presented. At the end, participants filled out the Goldsmith’s Musical Sophistication Index (GMSI) questionnaire to record participants’ musical background. Three additional questions were added to check whether participants had absolute pitch, hearing problems and whether they were familiar with the sounds presented.</p>
</sec>
</sec>
<sec id="sec019">
<title>Experiment 2</title>
<sec id="sec020">
<title>Participants</title>
<p>Sixty participants participated in experiment 2 (mean age = 25.9 years, SD = 9.15, 41 females). Participants included 36 undergraduate psychology students from Western Sydney University and the remaining 24 were again recruited through word-of-mouth and social media advertisements. The participant group identified as culturally diverse (35 Australians, 7 Europeans, 5 Asians, 8 Middle Eastern, 2 Hispanic, 1 American and 2 New Zealanders). Fourteen participants had participated in Experiment 1 as well, with a gap of two months between experiments. Participation was rewarded with either course credit or $20. Thirty-seven reported being able to play at least one musical instrument and 20 of those played for more than 5 years. Written informed consent was obtained from all participants prior to the start of the experiment, and the study was approved by the Western Sydney University Human Research Ethics Committee.</p>
</sec>
<sec id="sec021" sec-type="materials|methods">
<title>Materials</title>
<p>The generation of stimuli went through the exact same procedure as experiment 1, but the stimuli were this time presented in the equal temperament version of the BP scale.</p>
</sec>
<sec id="sec022">
<title>Procedure</title>
<p>The procedure of Experiment 2 was analogous to the first experiment. Participants who had participated previously were told that the experiment was slightly different, but were not informed about the nature of the difference.</p>
</sec>
</sec>
</sec>
<sec id="sec023">
<title>Statistical methods</title>
<sec id="sec024">
<title>Bayesian regression</title>
<p>In the Bayesian paradigm, probability is interpreted as a level of uncertainty, which is in contrast with the classical (frequentist) paradigm [<xref ref-type="bibr" rid="pone.0218570.ref058">58</xref>–<xref ref-type="bibr" rid="pone.0218570.ref059">59</xref>].</p>
<p>A multilevel Bayesian regression model was used to evaluate the effects of the independent variables on consonance and valence (i.e. pleasantness and happiness) ratings. Results were modelled and analysed in R [<xref ref-type="bibr" rid="pone.0218570.ref060">60</xref>] with the brms package using Stan [<xref ref-type="bibr" rid="pone.0218570.ref060">60</xref>–<xref ref-type="bibr" rid="pone.0218570.ref062">62</xref>]. The model used is the Bayesian version of a classical mixed effects model: it contains population-level effects (analogous to classical fixed effects) and group-level effects (analogous to classical random effects). In this case, the participants will be regarded as “group”, therefore the population-level effects are the estimated means of the effects across all participants tested; the group-level effects are the estimated standard deviations and correlations between the effects. An advantage of including group-level effects is that the model can account for differences between participants. For example, some participants may generally give lower ratings than others. Similarly, some participants may, for example, be relatively unfamiliar with Western music and so less sensitive to 12-TET Dissimilarity. At the same time, such a model can estimate the population-level effect of the “average” participant (given the pool of participants in the experiment). In comparison to models without group-level effects, models with group-level effects are less likely to provide evidence for non-existent effects (in non-Bayesian stats, this would be a Type I error) without sacrificing power [<xref ref-type="bibr" rid="pone.0218570.ref063">63</xref>].</p>
<p>The coefficients of a Bayesian regression have exactly the same meaning as in a standard regression: if the coefficient (effect) of predictor <italic>k</italic> is denoted <italic>β<sub>k</sub></italic>, a unit increase in that predictor is associated with a <italic>β<sub>k</sub></italic>-unit increase in the DV when all other predictors are held constant. A concise overview of Bayesian statistics in general is provided in Van de Schoot and Depaoli [<xref ref-type="bibr" rid="pone.0218570.ref059">59</xref>], whereas a more in-depth account can be found in Kruschke [<xref ref-type="bibr" rid="pone.0218570.ref064">64</xref>].</p>
<p>An important advantage of Bayesian regression is that, given the observed data and a prior distribution (see the next section for a <xref ref-type="sec" rid="sec032">discussion</xref> of priors), it calculates the whole posterior probability distribution of each effect’s size rather than only a point-estimate of the most probable effect size for each effect. A traditional power analysis, which assumes that a hypothesized parameter value is punctate, is in the Bayesian context therefore not applicable. This process allows for Bayesian credibility intervals to be calculated; unlike the confidence intervals in classical regression, credibility intervals have a straightforward and intuitive meaning: the 95% credibility interval of an effect size is the interval we can be 95% certain contains the effect’s true size.</p>
<p>Credibility intervals allow <italic>evidence ratios</italic> to be calculated; these are probability ratios (odds) in favour of directional hypotheses (such as a given effect being greater than zero): we term this directed hypothesis testing. For example, if the integral of the posterior distribution over the interval (0, ∞) is <italic>p</italic>, the evidence ratio in favour of the effect being greater than 0 is <italic>p</italic>/(1 –<italic>p</italic>); so, if the lower boundary of a (one-sided) 95% credibility interval is precisely zero, this implies there is a 5% probability the effect is less than zero and a 95% probability it is greater than zero, hence the evidence ratio is .95/.05 = 19.</p>
<p>To qualify the weight of evidence for or against any given hypothesis (e.g., that an effect is greater than 0), we follow the guidelines proposed by ([<xref ref-type="bibr" rid="pone.0218570.ref065">65</xref>] as cited by [<xref ref-type="bibr" rid="pone.0218570.ref066">66</xref>]): evidence ratios of 1–3 represent no evidence for the tested hypothesis; evidence ratios of 3–10 are “moderate” evidence for the hypothesis; evidence ratios of 10–30 are “strong” evidence; evidence ratios above 30 are “very strong” evidence. Evidence ratios close to zero represent evidence in favour of the null hypothesis.</p>
</sec>
<sec id="sec025">
<title>Priors</title>
<p>In Bayesian analyses, a prior distribution is chosen to reflect a parameter’s amount of (un)certainty before analysing the data [<xref ref-type="bibr" rid="pone.0218570.ref059">59</xref>]. A prior can, and should, reflect any prior knowledge, though as the amount of new data increases its influence in any case wanes. The priors used in all of our models are what are termed <italic>weakly informative priors</italic> [<xref ref-type="bibr" rid="pone.0218570.ref067">67</xref>–<xref ref-type="bibr" rid="pone.0218570.ref068">68</xref>] as there was little evidence about the consequences of the parameters included in the study. All of our non-binary predictors are standardized (they then have a standard deviation of 1; this suggests that we would be unlikely to see effect sizes with very high magnitudes).</p>
<p>For all population-level effects except the intercept, we used a prior with a Student’s <italic>t</italic>-distribution with 3 degrees of freedom, a mean of 0, and a scale of 2.5. Crucially, the zero mean indicates that our prior beliefs weakly favour the null hypothesis of zero effect size and–in comparison to using a flat uninformative prior–regularize the estimations, thereby reducing overfitting. Regularization is used to constrain a model’s flexibility in order to reduce uncertainty in the parameters by minimizing the <italic>mean square error</italic>.</p>
</sec>
<sec id="sec026">
<title>Models common to all experiments</title>
<p>The overall approach for analysing and modelling the results from both experiments is two-fold. First, a <italic>descriptive model</italic> of mean ratings per chord type and average pitch height (and average pitch height- squared) is fitted, to provide essential data visualization and a summary of the data. Second, a <italic>predictive model</italic> (a mixed-effects model) of the evaluative ratings with the intrinsic and extrinsic predictors outlined above is fitted to estimate the strengths of the proposed underlying factors. This procedure is done for Experiment 1 for both <italic>Consonance</italic> and <italic>Valence</italic>, subsequently for Experiment 2 and lastly, for the two experiments combined. In the interests of brevity, only the ratings and the predictive models for the two experiments combined will be reported in the main text. The separate analyses can be found in the supplementary materials.</p>
<p>Average chord ratings for each response variable were computed with rating as dependent variable and chord number (as factor), with average pitch height and average pitch height squared as independent variables.</p>
<p>Chord numbers were turned into factors and Chord 0 6 10 (numbers indicate BP semitone steps) was set as intercept. Chord 0 6 10 is the theoretical BP major chord and therefore might have the highest consonance and valence ratings, but the experimental results from Mathews et al. [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>] suggest otherwise. Chord ratings should therefore be interpreted with respect to this chord.</p>
<p>The pleasantness and valence ratings were separately modelled by the previously discussed intrinsic and extrinsic predictors: roughness (Roughness), harmonicity (Harmonicity), spectral entropy (SpectralEntropy), 12-TET Dissimilarity (12-TET Dissimilarity), average pitch height (AveragePitch) and average pitch height squared (AveragePitch^2), each of these interacting with musical sophistication (GMSI). All of the predictors were included as random effects. Stolzenburg’s periodicity measure [<xref ref-type="bibr" rid="pone.0218570.ref069">69</xref>] was tested as an alternative to harmonicity and was found–using leave-one-out cross-validation–to perform slightly worse at predicting out-of-sample data. All predictors and the ratings were standardized and scaled to their means except for Goldsmiths Musical Sophistication Index (GMSI), which was centred to the previously determined population mean from Müllensiefen et al. [<xref ref-type="bibr" rid="pone.0218570.ref057">57</xref>] (see Section <italic>Musical Sophistication</italic>). The 700 discrete values obtained from the slider ratings were modelled as being normally distributed, an assumption borne out by the posterior predictive checks shown in the supplementary materials. All the models converged as evidenced by Rhat. values below 1.</p>
</sec>
</sec>
<sec id="sec027" sec-type="results">
<title>Results</title>
<sec id="sec028">
<title>Just intonation and equal temperament</title>
<p>Two sets of participants have been exposed to two different tunings of the BP system, in contrast with the original study by Mathews et al. [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>] who solely tested the equal tempered version of the scale. In the just intonation version of the scale, we tested 414 chords due to the slightly unequal sizing of the 13 BP semitone steps. Before analysing whether participants perceived one tuning differently from the other, two models were fitted in order to find out whether grouping the 414 chords based on each different chord tuning into the 66 categories based purely on the chord’s step-size intervals was plausible. To clarify, by comparing these two models it is examined whether the 414 stimuli can be categorized into 66 units rather than 414. Both models included covariates of Average Pitch Height and Average Pitch Height-squared and all chord categories were referenced to Chord 0 6 10. This procedure was conducted for both the Consonance and Valence data. Approximate leave-one-out cross-validation was performed with LOO from the brms package [<xref ref-type="bibr" rid="pone.0218570.ref060">60</xref>–<xref ref-type="bibr" rid="pone.0218570.ref062">62</xref>] and it was demonstrated the simpler (66 grouping) model had a smaller LOO number than the more complex (414 grouping) model (shown in <xref ref-type="table" rid="pone.0218570.t003">Table 3</xref>) suggesting a better fit. This suggests that grouping the 414 chords in 66 categories is credible, and therefore only the best fitting model (with 66 chord groups) is presented for Experiment 1.</p>
<table-wrap id="pone.0218570.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.t003</object-id>
<label>Table 3</label> <caption><title>Testing whether the just intonation chords can best be considered as 414 items or 66 groups using the LOO Information Criterion.</title></caption>
<alternatives>
<graphic id="pone.0218570.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.t003" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="left"><italic>LOOIC</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Consonance414</td>
<td align="left">7716.61</td>
</tr>
<tr>
<td align="left">Consonance66</td>
<td align="left">7632.18</td>
</tr>
<tr>
<td align="left">Consonance414 -Consonance66</td>
<td align="left">84.43</td>
</tr>
<tr>
<td align="left">Valence414</td>
<td align="left">6201.39</td>
</tr>
<tr>
<td align="left">Valence66</td>
<td align="left">6020.22</td>
</tr>
<tr>
<td align="left">Valence414-Valence66</td>
<td align="left">181.17</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t003fn001"><p><italic>Notes</italic>. Consonance414 and Valence 414 represent the consonance and valence models with 414 chords, respectively. Consonance 66 and Valence66 represent the models with 66 chords, respectively. The table shows LOOIC (LOO Information Criterion).</p></fn>
</table-wrap-foot>
</table-wrap>
<p>After this procedure, it was analysed whether ratings in equal temperament differed from those in just intonation. Two models were fitted to compare consonance ratings in both tunings and valence ratings in both tunings with the chord ratings and tuning as categorical variables. There was no effect of tuning on the ratings, as the 95% Bayesian credibility intervals for tuning included zero. Therefore, we collapsed the two experiments and present the combined models of both.</p>
<p>An exploratory regression was conducted in order to estimate whether short-term statistical learning had taken place throughout the experiment, as such results have been previously found by Loui [<xref ref-type="bibr" rid="pone.0218570.ref032">32</xref>,<xref ref-type="bibr" rid="pone.0218570.ref033">33</xref>]. Besides, it helps to determine whether participation in related experiments affected ratings. TrialNo (accumulative trial numbers), TrialNo^2 (squared to allow for nonlinearity in the effects), ChordNo (the number of times participants have heard a particular chord) and ChordNo^2 were included as independent variables to test their effects on consonance and valence ratings. The resulting models are reported in the supplementary materials; in summary, predictors whose credibility intervals do not cross zero are reported here. TrialNo had a small effect on consonance ratings (= -.04). Small effects for valence ratings were found for ChordNo (= 0.11) and ChordNo^2 (= -.02). These results indicate that consonance ratings slightly decreased with an increase in trial number, which might suggest the presence of a boredom effect. Valence ratings went up slightly with an increase in chord number, which hints towards a subtle mere exposure effect. These weak effects do not suggest strong learning during the experiment, therefore participants who have participated in both will be included in the analyses.</p>
</sec>
<sec id="sec029">
<title>Descriptive models of consonance and valence ratings</title>
<p><xref ref-type="fig" rid="pone.0218570.g001">Fig 1</xref> shows the mean ratings for the 66 chords for consonance (left) and valence (right) for both experiments combined, and the effects of the predictors AveragePitch and AveragePitch^2. The model accounted for autoregressive effects of each participant’s previous response on their next response. The credibility intervals (illustrated by the thin blue lines) show the range of values within which there is a 95% probability that the observed ratings’ mean values fall. Average Pitch seems to have an effect on the overall ratings of the chords, suggesting that the higher the transposition of a chord, the more pleasant/happy it is rated. The size of the difference between the most consonant and least consonant is large (mean = 0.76). For valence, the size of the effect between the happiest and the least happy chord is also large (mean = 0.64). Importantly, the majority of responses are below zero, which indicates that the ratings are generally lower than for the reference item.</p>
<fig id="pone.0218570.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.g001</object-id>
<label>Fig 1</label>
<caption>
<title/>
<p>Mean consonance (left) and valence (right) ratings with 95% credibility intervals after controlling for a quadratic function of average pitch height. The thick darker blue line shows the 50% interval and the thinner light blue line shows the 95% interval.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.g001" xlink:type="simple"/>
</fig>
<p>Marginal effect plots of Average Pitch Height and histograms depicting the mean ratings across the 66 chords for Experiment 1 and 2 separately and combined can be found in the supplementary materials.</p>
<p>Correlational analyses were conducted to examine the relationship between the different predictors for consonance and valence ratings (<xref ref-type="fig" rid="pone.0218570.g002">Fig 2</xref>). For consonance, moderate positive linear relationships were found between Roughness and Spectral Entropy (r = .36), Harmonicity and Spectral Entropy (r = .39), and a moderate negative relationship was found for Roughness and Average Pitch (r = -.36). For valence, moderate positive linear relationships were found between Roughness and Spectral Entropy (r = .36), Harmonicity and Spectral Entropy (r = .39), Rating and Average Pitch (r = .41) and a moderate negative relationship was found for Roughness and Average Pitch (r = -.37).</p>
<fig id="pone.0218570.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.g002</object-id>
<label>Fig 2</label>
<caption>
<title/>
<p>Correlation matrix showing correlations between consonance ratings (left), valence ratings (right) and Roughness, Harmonicity, Spectral Entropy, 12-TET Dissimilarity, GMSI, Average Pitch, and Average Pitch^2.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec030">
<title>Predictive model of consonance</title>
<p><xref ref-type="fig" rid="pone.0218570.g003">Fig 3</xref> shows the 95% Bayesian credibility intervals of the predictor variables for a model of Consonance for both experiments combined. The predictors were also modelled for the experiments separately, but the results were very similar showing that the predictors generalize well between the subtle tuning differences and that they are consistent across experiments. For that reason, we focus here on both experiments combined (separate analyses of Experiment 1 and 2 can be found in the supplementary materials). As hypothesized, Roughness, Harmonicity, Spectral Entropy, 12-TET Dissimilarity and Average Pitch Height all predict consonance in the expected direction. The evidence ratios for the conditional main effects of the predictors are all “very strong”, as presented in <xref ref-type="table" rid="pone.0218570.t004">Table 4</xref>. Interesting to note is that there is also evidence to suggest that participants with more musical training gave higher pleasantness ratings. Strong evidence was obtained for interactions between Goldsmith’s Musical Sophistication Index (GMSI) and Harmonicity and GMSI and 12-TET Dissimilarity. The coefficients are small for Roughness (= -.14), Spectral Entropy (= -.11), 12-TET Dissimilarity (= -.12), Average Pitch (= .15) and GMSI:AveragePitch (= .11). The results of the Bayesian regression model indicated that the predictors explained 41.6% of the variance in the data.</p>
<fig id="pone.0218570.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Consonance model with all modelled parameters and 95% Bayesian credibility intervals for Experiment 1 and 2.</title>
<p>The thick darker blue line shows the 50% interval and the thinner light blue line shows the 95% interval.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.g003" xlink:type="simple"/>
</fig>
<table-wrap id="pone.0218570.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.t004</object-id>
<label>Table 4</label> <caption><title>Directed hypothesis testing for consonance.</title></caption>
<alternatives>
<graphic id="pone.0218570.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.t004" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Hypothesis</th>
<th align="left">Estimate</th>
<th align="left">Est. Error</th>
<th align="left">CI Lower</th>
<th align="left">CI Upper</th>
<th align="left">Evid. Ratio</th>
<th align="left">Star</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Roughness &lt; 0</td>
<td align="left">-0.14</td>
<td align="left">0.02</td>
<td align="left">-Inf.</td>
<td align="left">-0.11</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t004fn001">*</xref></td>
</tr>
<tr>
<td align="left">Harmonicity &gt; 0</td>
<td align="left">0.08</td>
<td align="left">0.01</td>
<td align="left">0.07</td>
<td align="left">Inf.</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t004fn001">*</xref></td>
</tr>
<tr>
<td align="left">SpectralEntropy &lt; 0</td>
<td align="left">-0.11</td>
<td align="left">0.02</td>
<td align="left">-Inf.</td>
<td align="left">-0.08</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t004fn001">*</xref></td>
</tr>
<tr>
<td align="left">12-TET Dissimilarity &lt; 0</td>
<td align="left">-0.12</td>
<td align="left">0.01</td>
<td align="left">-Inf.</td>
<td align="left">-0.10</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t004fn001">*</xref></td>
</tr>
<tr>
<td align="left">AveragePitch &gt; 0</td>
<td align="left">0.15</td>
<td align="left">0.03</td>
<td align="left">0.10</td>
<td align="left">Inf.</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t004fn001">*</xref></td>
</tr>
<tr>
<td align="left">AveragePitch^2 &gt; 0</td>
<td align="left">0.03</td>
<td align="left">0.01</td>
<td align="left">0.01</td>
<td align="left">Inf.</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t004fn001">*</xref></td>
</tr>
<tr>
<td align="left">GMSI:Roughness &lt; 0</td>
<td align="left">-0.02</td>
<td align="left">0.03</td>
<td align="left">-Inf.</td>
<td align="left">0.03</td>
<td align="left">2.55</td>
<td align="left"/>
</tr>
<tr>
<td align="left">GMSI:Harmonicity &gt; 0</td>
<td align="left">0.07</td>
<td align="left">0.02</td>
<td align="left">0.04</td>
<td align="left">Inf.</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t004fn001">*</xref></td>
</tr>
<tr>
<td align="left">GMSI:SpectralEntropy &lt; 0</td>
<td align="left">0.03</td>
<td align="left">0.03</td>
<td align="left">-0.02</td>
<td align="left">Inf.</td>
<td align="left">5.51</td>
<td align="left"/>
</tr>
<tr>
<td align="left">GMSI:12-TET Dissimilarity &gt; 0</td>
<td align="left">-0.06</td>
<td align="left">0.02</td>
<td align="left">-Inf.</td>
<td align="left">-0.03</td>
<td align="left">1499.00</td>
<td align="left"><xref ref-type="table-fn" rid="t004fn001">*</xref></td>
</tr>
<tr>
<td align="left">GMSI:AveragePitch &lt; 0</td>
<td align="left">-0.11</td>
<td align="left">0.06</td>
<td align="left">-Inf.</td>
<td align="left">0.01</td>
<td align="left">29.46</td>
<td align="left"><xref ref-type="table-fn" rid="t004fn001">*</xref></td>
</tr>
<tr>
<td align="left">GMSI:AveragePitch^2 &gt; 0</td>
<td align="left">0.02</td>
<td align="left">0.02</td>
<td align="left">0.00</td>
<td align="left">Inf.</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t004fn001">*</xref></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t004fn001"><p>* = The expected value under the hypothesis lies outside the 95%-CI (credibility intervals). Estimate = mean.</p></fn>
<fn id="t004fn002"><p>Estimate error = standard deviation of the posterior distribution. CI lower and CI upper are two-sided 95% credibility intervals. Evidence ratio = the posterior probability under the hypothesis against its alternative.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The marginal effects plots in <xref ref-type="fig" rid="pone.0218570.g004">Fig 4</xref> show the impact of the moderating effect of GMSI on Harmonicity and 12-TET Dissimilarity. GMSI does not interact with the effects of the other predictors.</p>
<fig id="pone.0218570.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Marginal effects plots for GMSI and Harmonicity and GMSI and 12-TET Dissimilarity.</title>
<p>Levels of GMSI represent the mean (0), 1 SD above the mean (1) and 1 SD below the mean (-1).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.g004" xlink:type="simple"/>
</fig>
<p>The Harmonicity plot shows that, as hypothesized, chords with higher levels of harmonicity are perceived as more pleasant. Furthermore, this effect is stronger for musically sophisticated participants than it is for musically unsophisticated participants. The 12-TET Dissimilarity plot illustrates that the more distant a chord is from 12-TET, the less pleasant it is (this is as hypothesized); again, this effect is stronger for musically sophisticated participants than for musically unsophisticated participants.</p>
</sec>
<sec id="sec031">
<title>Predictive model of valence</title>
<p><xref ref-type="fig" rid="pone.0218570.g005">Fig 5</xref> shows the 95% Bayesian credibility intervals of the independent variables for Valence for both experiments combined. As hypothesized, Roughness, Harmonicity, Spectral Entropy, 12-TET Dissimilarity and Average Pitch Height all predict consonance in the expected direction. The evidence ratios for the conditional main effects of the predictors are all “very strong”, as presented in <xref ref-type="table" rid="pone.0218570.t005">Table 5</xref>. Strong evidence was obtained for interactions between GMSI and Harmonicity and between GMSI and Average Pitch. The coefficient for Average Pitch is medium (= .4) and small for GMSI:Average Pitch (= -.12). Smaller coefficients are found for the other predictors, namely Roughness (= -.08), Harmonicity (= .06), Spectral Entropy (= -.06) and 12-TET Dissimilarity (= -.04). The effect of Average Pitch Height is quite striking as it is very strong in comparison to the other parameters and to its effect on the consonance ratings. The results of the Bayesian regression model indicated that the predictors explained 47.4% of the data.</p>
<fig id="pone.0218570.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Valence model with all modelled parameters and 95% Bayesian credibility intervals for Experiment 1 and 2.</title>
<p>The thick darker blue line shows the 50% interval and the thinner light blue line shows the 95% interval.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.g005" xlink:type="simple"/>
</fig>
<table-wrap id="pone.0218570.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.t005</object-id>
<label>Table 5</label> <caption><title>Directed hypothesis testing for valence.</title></caption>
<alternatives>
<graphic id="pone.0218570.t005g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.t005" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Hypothesis</th>
<th align="left">Estimate</th>
<th align="left">Est. Error</th>
<th align="left">CI Lower</th>
<th align="left">CI Upper</th>
<th align="left">Evid. Ratio</th>
<th align="left">Star</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Roughness &lt; 0</td>
<td align="left">-0.08</td>
<td align="left">0.01</td>
<td align="left">-Inf.</td>
<td align="left">-0.06</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t005fn001">*</xref></td>
</tr>
<tr>
<td align="left">Harmonicity &gt; 0</td>
<td align="left">0.06</td>
<td align="left">0.01</td>
<td align="left">0.05</td>
<td align="left">Inf.</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t005fn001">*</xref></td>
</tr>
<tr>
<td align="left">SpectralEntropy &lt; 0</td>
<td align="left">-0.06</td>
<td align="left">0.01</td>
<td align="left">-Inf.</td>
<td align="left">-0.04</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t005fn001">*</xref></td>
</tr>
<tr>
<td align="left">12-TET Dissimilarity &lt; 0</td>
<td align="left">-0.04</td>
<td align="left">0.01</td>
<td align="left">-Inf.</td>
<td align="left">-0.02</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t005fn001">*</xref></td>
</tr>
<tr>
<td align="left">AveragePitch &gt; 0</td>
<td align="left">0.40</td>
<td align="left">0.03</td>
<td align="left">0.35</td>
<td align="left">Inf.</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t005fn001">*</xref></td>
</tr>
<tr>
<td align="left">AveragePitch^2 &gt; 0</td>
<td align="left">0.16</td>
<td align="left">0.02</td>
<td align="left">0.12</td>
<td align="left">Inf.</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t005fn001">*</xref></td>
</tr>
<tr>
<td align="left">GMSI:Roughness &lt; 0</td>
<td align="left">-0.03</td>
<td align="left">0.02</td>
<td align="left">-Inf.</td>
<td align="left">0.01</td>
<td align="left">8.11</td>
<td align="left"/>
</tr>
<tr>
<td align="left">GMSI:Harmonicity &gt; 0</td>
<td align="left">0.03</td>
<td align="left">0.01</td>
<td align="left">0.00</td>
<td align="left">Inf.</td>
<td align="left">38.13</td>
<td align="left"><xref ref-type="table-fn" rid="t005fn001">*</xref></td>
</tr>
<tr>
<td align="left">GMSI:SpectralEntropy &lt; 0</td>
<td align="left">0.00</td>
<td align="left">0.02</td>
<td align="left">-0.04</td>
<td align="left">Inf.</td>
<td align="left">0.68</td>
<td align="left"/>
</tr>
<tr>
<td align="left">GMSI:12-TET Dissimilarity &gt; 0</td>
<td align="left">0.00</td>
<td align="left">0.01</td>
<td align="left">-Inf.</td>
<td align="left">0.02</td>
<td align="left">1.34</td>
<td align="left"/>
</tr>
<tr>
<td align="left">GMSI:AveragePitch &lt; 0</td>
<td align="left">-0.12</td>
<td align="left">0.05</td>
<td align="left">-Inf.</td>
<td align="left">-0.03</td>
<td align="left">64.69</td>
<td align="left"><xref ref-type="table-fn" rid="t005fn001">*</xref></td>
</tr>
<tr>
<td align="left">GMSI:AveragePitch^2 &gt; 0</td>
<td align="left">0.02</td>
<td align="left">0.01</td>
<td align="left">0.00</td>
<td align="left">Inf.</td>
<td align="left">Inf.</td>
<td align="left"><xref ref-type="table-fn" rid="t005fn001">*</xref></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t005fn001"><p>* = The expected value under the hypothesis lies outside the 95%-CI (credibility intervals). Estimate = mean.</p></fn>
<fn id="t005fn002"><p>Estimate error = standard deviation of the posterior distribution. CI lower and CI upper are two-sided 95% credibility intervals. Evidence ratio = the posterior probability under the hypothesis against its alternative.</p></fn>
</table-wrap-foot>
</table-wrap>
<p><xref ref-type="fig" rid="pone.0218570.g006">Fig 6</xref> shows the marginal plots for the interaction between GMSI and Harmonicity and between GMSI and Average Pitch. This interaction shows a similar pattern as the previously discussed interaction between GMSI and Harmonicity in the Consonance model. Chords with higher harmonicity levels were rated happier by participants with a high GMSI.</p>
<fig id="pone.0218570.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Marginal effects plots for GMSI and Harmonicity and GMSI and Average Pitch.</title>
<p>Levels of GMSI represent the mean (0), 1 SD above the mean (1) and 1 SD below the mean (-1).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.g006" xlink:type="simple"/>
</fig>
<p>Kendall’s rank correlation was conducted to see whether the consonance and valence ratings are associated. Based on the results of this study, consonance ratings are independent from valence ratings <italic>r</italic><sub><italic>τ</italic></sub> = .14, <italic>p &lt;</italic> .05. Out of the 66 chords, Bonferroni corrected dependent Wilcoxon signed-rank tests demonstrated that a total of 7 chords were rated significantly differently in the consonance versus the valence condition (<italic>p &lt;</italic> .05).</p>
</sec>
</sec>
<sec id="sec032" sec-type="conclusions">
<title>Discussion</title>
<p>The current study examined the influence of intrinsic features on affective responses to unfamiliar musical stimuli. In two experiments, participants rated the consonance and valence of chords from the Bohlen-Pierce system, and a set of intrinsic features (roughness, harmonicity, spectral entropy and average pitch height) and one extrinsic feature (familiarity) were used to predict their responses. Roughness was chosen as a well-established, but somewhat disputed, predictor of music affect. Harmonicity is a familiar concept, but has a variety of mathematical formulations; it was calculated here in a novel way using expectation tensors. Spectral entropy is usually a predictor of a proposed timbral dimension, and has never been used as a predictor of consonance. 12-TET Dissimilarity is a straightforward but novel predictor, and average pitch height and average pitch height squared complete the set of predictors used. All these predictors were combined with musical sophistication as assessed by the Goldsmiths Musical Sophistication Index. By using such a large set of both novel and established predictors, a systematic approach was taken, estimating the independent effects of each variable and how it is moderated by musical sophistication.</p>
<p>The results from both experiments were consistent and suggest the following notable findings: (1) consistently across participants, some chords from the BP system have distinctly different ratings of consonance and valence; (2) the subtle changes between the two tuning systems did not have an impact on participants’ ratings; (3) the two predictive models for consonance and valence are quite similar, apart from the effect of Average Pitch height, which differs in degree rather than direction; (4) there is very strong evidence that all of the intrinsic predictors in the models are independently correlated with the ratings as hypothesized.</p>
<sec id="sec033">
<title>Chord ratings</title>
<p>Similar to the original study of Mathews et al. [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>], a large range of consonance was observed. Since we did not include the tritave in the chord selection as was done in Mathews et al. [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>], a direct comparison between the most consonant and dissonant chords cannot be made, but several chords do appear to have strong ratings in both studies, for example Chords 0 6 10 (the BP major chord), 0 5 11, 0 3 5 and 0 8 11. Note that in their study, results were split into two participant groups, namely musicians and non-musicians. Chord 0 5 11 and 0 3 5 were among the highest rated chords for musicians, whereas Chord 0 6 10 and 0 8 11 scored high in the non-musician group. An important implication that can be drawn from the chord ratings overall, from both this study and Mathews et al.’s study [<xref ref-type="bibr" rid="pone.0218570.ref026">26</xref>], is that <italic>previously unheard</italic> BP chords are rated <italic>consistently</italic> across participants. Furthermore, the highest and the lowest rated chords are consistent across participants. Specifically, Chord 0 5 11 received very high ratings for both consonance and valence in both experiments. Can this be explained by any of the intrinsic factors or is this purely caused by its intervals being of close proximity to Western intervals? Several high and low rated chords and their respective Roughness, Harmonicity, 12-TET Dissimilarity and Spectral Entropy values, as well as descriptive statistics for those predictors, are presented in Tables <xref ref-type="table" rid="pone.0218570.t006">6</xref>–<xref ref-type="table" rid="pone.0218570.t008">8</xref>. For the highest rated chords, Chord 0 5 11 reaches a harmonicity value more than 1 standard deviation higher than the mean and Chord 0 8 11 is close to that as well. Apart from Chord 0 3 11 and 0 6 11, all other chords have harmonicity values in the third quartile, and exemplify the role of harmonicity for the perception of positive affect. Spectral Entropy, 12-TET Dissimilarity and Roughness values do not show a clear pattern, suggesting that the high ratings obtained for these chords are not due to those factors.</p>
<table-wrap id="pone.0218570.t006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.t006</object-id>
<label>Table 6</label> <caption><title>Descriptive statistics for roughness, harmonicity, spectral entropy, and 12-TET dissimilarity.</title></caption>
<alternatives>
<graphic id="pone.0218570.t006g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.t006" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="left">Roughness</th>
<th align="left">Harmonicity</th>
<th align="left">Spectral Entropy</th>
<th align="left">12-TET Dissimilarity</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Minimum</td>
<td align="left">-0.69</td>
<td align="left">-0.60</td>
<td align="left">-1.31</td>
<td align="left">-1.10</td>
</tr>
<tr>
<td align="left">1<sup>St</sup> Quartile</td>
<td align="left">-0.41</td>
<td align="left">-0.33</td>
<td align="left">-0.33</td>
<td align="left">-0.35</td>
</tr>
<tr>
<td align="left">Median</td>
<td align="left">-0.12</td>
<td align="left">-0.12</td>
<td align="left">0.13</td>
<td align="left">-0.09</td>
</tr>
<tr>
<td align="left">Mean</td>
<td align="left">0.00</td>
<td align="left">0.00</td>
<td align="left">-0.00</td>
<td align="left">-0.00</td>
</tr>
<tr>
<td align="left">3<sup>rd</sup> Quartile</td>
<td align="left">0.28</td>
<td align="left">0.09</td>
<td align="left">0.25</td>
<td align="left">0.34</td>
</tr>
<tr>
<td align="left">Maximum</td>
<td align="left">1.58</td>
<td align="left">2.16</td>
<td align="left">0.77</td>
<td align="left">1.20</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t006fn001"><p><italic>Notes</italic>. Values are standardized.</p></fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="pone.0218570.t007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.t007</object-id>
<label>Table 7</label> <caption><title>Roughness, harmonicity, spectral entropy and 12-TET dissimilarity values for several high rated chords for consonance and valence.</title></caption>
<alternatives>
<graphic id="pone.0218570.t007g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.t007" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="left">Roughness</th>
<th align="left">Harmonicity</th>
<th align="left">Spectral Entropy</th>
<th align="left">12-TET Dissimilarity</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Chord 0 3 5</td>
<td align="left">0.54</td>
<td align="left">2.16</td>
<td align="left">0.35</td>
<td align="left">0.08</td>
</tr>
<tr>
<td align="left">Chord 0 3 11</td>
<td align="left">-0.34</td>
<td align="left">-0.25</td>
<td align="left">0.18</td>
<td align="left">-0.33</td>
</tr>
<tr>
<td align="left">Chord 0 5 11</td>
<td align="left">-0.54</td>
<td align="left">1.18</td>
<td align="left">0.12</td>
<td align="left">-0.27</td>
</tr>
<tr>
<td align="left">Chord 0 6 11</td>
<td align="left">-0.54</td>
<td align="left">0.00</td>
<td align="left">-0.17</td>
<td align="left">-0.90</td>
</tr>
<tr>
<td align="left">Chord 0 8 11</td>
<td align="left">-0.05</td>
<td align="left">0.92</td>
<td align="left">-0.35</td>
<td align="left">-0.33</td>
</tr>
<tr>
<td align="left">Chord 0 6 10 (Intercept)</td>
<td align="left">-0.44</td>
<td align="left">2.16</td>
<td align="left">-0.29</td>
<td align="left">0.08</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t007fn001"><p><italic>Notes</italic>. Values are standardized.</p></fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="pone.0218570.t008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0218570.t008</object-id>
<label>Table 8</label> <caption><title>Roughness, harmonicity, spectral entropy and 12-TET dissimilarity values for several low rated chords for consonance and valence.</title></caption>
<alternatives>
<graphic id="pone.0218570.t008g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.t008" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="left">Roughness</th>
<th align="left">Harmonicity</th>
<th align="left">Spectral Entropy</th>
<th align="left">12-TET Dissimilarity</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Chord 0 1 2</td>
<td align="left">1.58</td>
<td align="left">0.19</td>
<td align="left">-0.27</td>
<td align="left">-0.33</td>
</tr>
<tr>
<td align="left">Chord 0 1 9</td>
<td align="left">0.23</td>
<td align="left">-0.50</td>
<td align="left">-0.08</td>
<td align="left">0.19</td>
</tr>
<tr>
<td align="left">Chord 0 1 10</td>
<td align="left">0.04</td>
<td align="left">-0.30</td>
<td align="left">-0.78</td>
<td align="left">0.60</td>
</tr>
<tr>
<td align="left">Chord 0 8 9</td>
<td align="left">0.04</td>
<td align="left">-0.36</td>
<td align="left">-0.30</td>
<td align="left">0.25</td>
</tr>
<tr>
<td align="left">Chord 0 9 10</td>
<td align="left">-0.17</td>
<td align="left">0.03</td>
<td align="left">0.50</td>
<td align="left">0.19</td>
</tr>
<tr>
<td align="left">Chord 0 6 10 (Intercept)</td>
<td align="left">-0.44</td>
<td align="left">2.16</td>
<td align="left">-0.29</td>
<td align="left">0.08</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t008fn001"><p><italic>Notes</italic>. Values are standardized.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Consideration of the lowest rated chords in <xref ref-type="table" rid="pone.0218570.t008">Table 8</xref>, reveals a maximum Roughness value is obtained for Chord 0 1 2. However, this chord was only among the lowest rated chords for Consonance, but not for Valence. Spectral entropy is quite high for Chord 0 9 10 in comparison with the highest rated chords. The pattern among the predictors is less obvious for the lowest-rated chords. This could mean that perception of consonance and valence is a more complex process involving a combination of the factors tested in this study, nonlinearities in their effects, or perhaps other predictors that have not been modelled.</p>
</sec>
<sec id="sec034">
<title>Intrinsic and extrinsic predictors</title>
<p>Apart from demonstrating that unfamiliar chords can evoke different affective responses, this study has provided very strong evidence that all the predictors have an effect in the expected direction. Furthermore, the effects are consistent across the two experiments despite slightly different tunings, and broadly consistent across the two affective responses. They are also consistent in direction across musical expertise. This study provides strong evidence that intrinsic features influence affective responses–consonance and valence–to music. To reiterate, intrinsic features were defined as features that do not rely on long-term statistical regularities and that have the potential to function across different musical cultures.</p>
<p>The role of roughness and harmonicity in consonance perception has been thoroughly tested and criticized over the years in numerous theoretical and experimental studies (e.g. [<xref ref-type="bibr" rid="pone.0218570.ref035">35</xref>,<xref ref-type="bibr" rid="pone.0218570.ref003">3</xref>–<xref ref-type="bibr" rid="pone.0218570.ref005">5</xref>,<xref ref-type="bibr" rid="pone.0218570.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0218570.ref070">70</xref>–<xref ref-type="bibr" rid="pone.0218570.ref071">71</xref>], but there are many more). Roughness was found to be negatively correlated with pleasantness and happiness ratings, and this effect was not moderated by musical sophistication. This suggests, in contrast to McLachlan et al. [<xref ref-type="bibr" rid="pone.0218570.ref038">38</xref>] that perception of roughness in unfamiliar chords is not affected by a musical training effect.</p>
<p>Similar to findings of McDermott et al. [<xref ref-type="bibr" rid="pone.0218570.ref043">43</xref>], an interaction was found in both experiments between harmonicity and musical sophistication in predicting consonance and in predicting valence ratings in the second experiment. Causal explanations for this moderation include the possibility of musical experience amplifying preferences for harmonicity (as suggested by McDermott et al. [<xref ref-type="bibr" rid="pone.0218570.ref043">43</xref>]) or that people who are sensitive to harmonicity are drawn to music and hence have higher musical sophistication. The latter might, however, suggest a similar moderation effect occurring for roughness, for which there was only limited evidence in our data.</p>
<p>Spectral entropy is a novel predictor for the consonance and valence of chords. It was found to be a useful predictor, in the expected direction in all models tested, both for consonance and valence. As mentioned earlier, spectral entropy is a way to quantify the unpredictability of the spectrum of the chord. An implication of a highly unpredictable chord spectrum is that it will evoke uncertainty about which pitches are to be perceived or how these fit to a harmonic series [<xref ref-type="bibr" rid="pone.0218570.ref070">70</xref>]. Complex and unpredictable spectra are more difficult to perceive and process. A chord with high spectral entropy could therefore negatively affect processing fluency which would subsequently lead to a more negative aesthetic experience (such as the perception of dissonance).</p>
<p>Another notable finding of this study is the strong effect of average pitch height in both models, supporting the theory that high pitch is associated with positive affect, with positive applying to both pleasantness and happiness [<xref ref-type="bibr" rid="pone.0218570.ref028">28</xref>,<xref ref-type="bibr" rid="pone.0218570.ref051">51</xref>–<xref ref-type="bibr" rid="pone.0218570.ref052">52</xref>,<xref ref-type="bibr" rid="pone.0218570.ref031">31</xref>]. These correlations may partly be explained by the relationship between music and vocal affect expression [<xref ref-type="bibr" rid="pone.0218570.ref035">35</xref>,<xref ref-type="bibr" rid="pone.0218570.ref072">72</xref>–<xref ref-type="bibr" rid="pone.0218570.ref073">73</xref>]. The strong effect of pitch height on valence ratings is not surprising, as pitch height is often associated with happiness in music [<xref ref-type="bibr" rid="pone.0218570.ref028">28</xref>,<xref ref-type="bibr" rid="pone.0218570.ref051">51</xref>–<xref ref-type="bibr" rid="pone.0218570.ref052">52</xref>,<xref ref-type="bibr" rid="pone.0218570.ref031">31</xref>], but interestingly, consonance ratings were also partially predicted by pitch height. However, the strength of pitch height on valence responses was considerably larger than on consonance responses.</p>
<p>Possibly, pitch height could be interpreted being a musical ‘universal’ as those with lower levels of musical sophistication nonetheless instantly recognize it as happy. An interaction between musical sophistication and average pitch was only found in the valence model. Even though it is a fairly small effect, it was shown that less musically sophisticated participants were more affected by a higher average pitch.</p>
<p>A limitation of this study is the over-representation of first-year psychology students in the participant pool. Even though a separate recruitment procedure was conducted, the overall majority of the participants were recruited as part of their psychology course requirements, resulting in limitations in, for example, age diversity. However, the multicultural community that can be found at Western Sydney University ensured a culturally and linguistically diverse participant pool.</p>
<p>Also, this study examined a specific set of theoretically motivated intrinsic predictors while including only a single extrinsic predictor because of the unfamiliar nature of the stimuli. Although the study showed that this specific set of predictors have an impact on affective responses to music, the Bayesian R-squared of .41 for consonance and .47 for valence show that the majority of variance remains unexplained; for this reason, it would be interesting and useful to develop additional new predictors.</p>
</sec>
</sec>
<sec id="sec035" sec-type="conclusions">
<title>Conclusion</title>
<p>This study aimed to present a systematic approach to examine perception of consonance and valence of an unfamiliar musical system. A number of novel and established intrinsic and extrinsic predictors were mathematically modelled. The results and models have given novel insights into how consonance and valence are perceived in the simple musical elements of chords. Crucially, we obtain strong evidence that a number of features intrinsic to an unfamiliar musical system impact on affective responses.</p>
<p>Similar to Milne et al. [<xref ref-type="bibr" rid="pone.0218570.ref042">42</xref>], effects of interval familiarity were minimized through the experimental design and the use of an unfamiliar microtonal system. Therefore, the effects of the intrinsic predictors found on consonance and valence ratings can be credibly attributed to the musical stimuli themselves, rather than to familiarity or learned associations. Altogether, the results indicate that, in an unfamiliar musical system, it is possible to show the important influence of intrinsic predictors on the perception of consonance and valence. This is in contrast with much current research in music perception, which argues that extrinsic or learned processes are of more importance than intrinsic predictors (e.g. [<xref ref-type="bibr" rid="pone.0218570.ref015">15</xref>–<xref ref-type="bibr" rid="pone.0218570.ref016">16</xref>]). Our results also suggest promising directions for future research in the field of music perception and affect: by moving away from commonly used Western musical stimuli, we can show that–in addition to long-term statistical learning of a musical system and its cultural context–fundamental perceptual mechanisms are at the heart of musical communication. The extent to which experience with a musical system might supersede these influences remains to be determined. Future experiments will consider further extrinsic aspects such as the effect of manipulating exposure to the system and a reductive causal intervention experiment including continuous affective responses to longer fragments of music in a Western tuning and a Bohlen-Pierce tuning.</p>
</sec>
<sec id="sec036">
<title>Supporting information</title>
<supplementary-material id="pone.0218570.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title/>
<p>Consonance (left column) and Valence (right column) for Experiment 1 (top), Experiment 2 (middle) and Experiment 1&amp;2 combined (bottom).</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title/>
<p>Consonance (left column) and Valence (right column) for Experiment 1 (top), Experiment 2 (middle) and Experiment 1&amp;2 combined (bottom). The histograms show whether ratings are skewed or normal in their distribution.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title/>
<p>Consonance (left column) and Valence (right column) for Experiment 1 (top), Experiment 2 (middle) and Experiment 1&amp;2 combined (bottom). The thin black line is the distribution of the observed outcomes and the blue lines represent the 1000 draws from the posterior predictive distribution.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title/>
<p>Mean consonance (left) and valence (right) ratings with 95% credibility intervals after controlling for a quadratic function of average pitch height. The thick darker blue line shows the 50% interval and the thinner light blue line shows the 95% interval.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Consonance model with all modelled parameters and 95% Bayesian credibility intervals for Experiment 1.</title>
<p>The thick darker blue line shows the 50% interval and the thinner light blue line shows the 95% interval.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Marginal effects plots for GMSI and Harmonicity and GMSI and 12-TET Dissimilarity.</title>
<p>Levels of GMSI represent the mean (0), 1 SD above the mean (1) and 1 SD below the mean (-1).</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s007" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title>Valence model with all modelled parameters and 95% Bayesian credibility intervals for Experiment 1.</title>
<p>The thick darker blue line shows the 50% interval and the thinner light blue line shows the 95% interval.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s008" xlink:type="simple">
<label>S8 Fig</label>
<caption>
<title/>
<p>Mean consonance (left) and valence (right) ratings with 95% credibility intervals after controlling for a quadratic function of average pitch height. The thick darker blue line shows the 50% interval and the thinner light blue line shows the 95% interval.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s009" xlink:type="simple">
<label>S9 Fig</label>
<caption>
<title>Consonance model with all modelled parameters and 95% Bayesian credibility intervals for Experiment 2.</title>
<p>The thick darker blue line shows the 50% interval and the thinner light blue line shows the 95% interval.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s010" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s010" xlink:type="simple">
<label>S10 Fig</label>
<caption>
<title>Marginal effects plots for GMSI and Harmonicity and GMSI and 12-TET Dissimilarity.</title>
<p>Levels of GMSI represent the mean (0), 1 SD above the mean (1) and 1 SD below the mean (-1).</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s011" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s011" xlink:type="simple">
<label>S11 Fig</label>
<caption>
<title>Valence model with all modelled parameters and 95% Bayesian credibility intervals for Experiment 2.</title>
<p>The thick darker blue line shows the 50% interval and the thinner light blue line shows the 95% interval.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s012" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s012" xlink:type="simple">
<label>S12 Fig</label>
<caption>
<title>Marginal effects plot for GMSI and Harmonicity.</title>
<p>Levels of GMSI represent the mean (0), 1 SD above the mean (1) and 1 SD below the mean (-1).</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0218570.s013" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pone.0218570.s013" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Supplementary materials.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pone.0218570.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lahdelma</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Eerola</surname> <given-names>T</given-names></name>. <article-title>Single chords convey distinct emotional qualities to both naïve and expert listeners</article-title>. <source>Psychology of Music</source>. <year>2016</year>, <volume>44</volume>(<issue>1</issue>):<fpage>37</fpage>–<lpage>54</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marin</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>, <name name-style="western"><surname>Gingras</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Stewart</surname> <given-names>L</given-names></name>. (2015). <article-title>Affective evaluation of simultaneous tone combinations in congenital amusia</article-title>. <source>Neuropsychologia</source>. <year>2015</year> <month>Nov</month>,<volume>78</volume>:<fpage>207</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2015.10.004" xlink:type="simple">10.1016/j.neuropsychologia.2015.10.004</ext-link></comment> <object-id pub-id-type="pmid">26455803</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kameoka</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kuriyagawa</surname> <given-names>M</given-names></name>. <article-title>Consonance theory part I: Consonance of dyads</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1969</year>, <volume>45</volume>:<fpage>1452</fpage>–<lpage>59</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kameoka</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kuriyagawa</surname> <given-names>M</given-names></name>. <article-title>Consonance theory part II: Consonance of complex tones and its calculation method</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1969</year>, <volume>45</volume>:<fpage>1460</fpage>–<lpage>69</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1911624" xlink:type="simple">10.1121/1.1911624</ext-link></comment> <object-id pub-id-type="pmid">5803169</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Plomp</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Levelt</surname> <given-names>WJM</given-names></name>. <article-title>Tonal consonance and critical bandwidth</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1965</year>, <volume>38</volume>(<issue>4</issue>):<fpage>548</fpage>–<lpage>560</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1909741" xlink:type="simple">10.1121/1.1909741</ext-link></comment> <object-id pub-id-type="pmid">5831012</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Crowder</surname> <given-names>RG</given-names></name>. <article-title>Perception of the major/minor distinction: I. Historical and theoretical foundations</article-title>. <source>Pyschomusicology</source>. <year>1984</year> <month>Jan</month> <day>1</day>,<volume>4</volume>(<issue>1–2</issue>):<fpage>3</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Crowder</surname> <given-names>RG</given-names></name>. <article-title>Perception of the major/minor distinction: II. Experimental investigations</article-title>. <source>Psychomusicology</source>, <year>1985</year> <month>Jan</month> <day>1</day>,<volume>5</volume>(<issue>1–2</issue>):<fpage>3</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Heinlein</surname> <given-names>CP</given-names></name>. <article-title>The affective characters of the major and minor modes in music</article-title>. <source>Journal of Computational Psychology</source>. <year>1928</year>,<volume>8</volume>:<fpage>101</fpage>–<lpage>42</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balkwill</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>. <article-title>A cross-cultural investigation of the perception of emotion in music: Psychophysical and cultural cues</article-title>. <source>Music Perception</source>. <year>1999</year>;<volume>17</volume>(<issue>1</issue>):<fpage>43</fpage>–<lpage>64</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balkwill</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>, <name name-style="western"><surname>Matsanuga</surname> <given-names>R</given-names></name>. <article-title>Recognition of emotion in Japanese, Western, and Hindustani music by Japanese listeners</article-title>. <source>Japanese Psychological Research</source>. <year>2004</year> <month>Nov</month> <day>25</day>,<volume>46</volume>(<issue>4</issue>):<fpage>337</fpage>–<lpage>349</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Västfjäll</surname> <given-names>D</given-names></name>. <article-title>Emotional responses to music: The need to consider underlying mechanisms</article-title>. <source>Behavioural and Brain Sciences</source>. <year>2008</year> <month>Oct</month>,<volume>31</volume>:<fpage>559</fpage>–<lpage>621</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref012"><label>12</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Cohen</surname> <given-names>AJ</given-names></name>. <chapter-title>Music as a source of emotion in film</chapter-title>: In: <name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Sloboda</surname> <given-names>JA</given-names></name>, editors. <source>Music and emotion: Theory and research</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2001</year>. p. <fpage>249</fpage>–<lpage>72</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pearce</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Wiggins</surname> <given-names>GA</given-names></name>. <article-title>Expectation in melody: The influence of context and learning</article-title>. <source>Music Perception</source>. <year>2006</year> <month>Jun</month>,<volume>23</volume>(<issue>5</issue>):<fpage>377</fpage>–<lpage>406</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Milne</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Laney</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sharp</surname> <given-names>DB</given-names></name>. <article-title>A spectral pitch class model of the probe tone data and scalic tonality</article-title>. <source>Music Perception</source>. <year>2015</year> <month>Apr</month>,<volume>32</volume>(<issue>4</issue>):<fpage>364</fpage>–<lpage>93</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cazden</surname> <given-names>N</given-names></name>. <article-title>Musical consonance and dissonance: A cultural criterion</article-title>. <source>The Journal of Aesthetics and Art Criticism</source>. <year>1945</year> <month>Sep</month>,<volume>4</volume>(<issue>1</issue>):<fpage>3</fpage>–<lpage>11</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cazden</surname> <given-names>N</given-names></name>. <article-title>The definition of consonance and dissonance</article-title>. <source>International Review of the Aesthetics and Sociology of Music</source>. <year>1980</year> <month>Dec</month>,<volume>11</volume>(<issue>2</issue>):<fpage>123</fpage>–<lpage>68</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parncutt</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Reisinger</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Fuchs</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kaiser</surname> <given-names>F</given-names></name>. <article-title>Consonance and prevalence of sonorities in Western polyphony: Roughness, harmonicity, familiarity, evenness, diatonicity</article-title>. <source>Journal of New Music Research</source>. <year>2019</year>, <volume>48</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cousineau</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McDermott</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Peretz</surname> <given-names>I</given-names></name>. <article-title>The basis of musical consonance as revealed by congenital amusia</article-title>. <source>PNAS</source>. <year>2012</year> <month>Nov</month> <day>27</day>,<volume>109</volume>(<issue>40</issue>):<fpage>19858</fpage>–<lpage>63</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McDermott</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>AF</given-names></name>, <name name-style="western"><surname>Undurraga</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Godoy</surname> <given-names>RA</given-names></name>. <article-title>Indifference to dissonance in native Amazonians reveals cultural variation in music perception</article-title>. <source>Nature</source>. <year>2016</year> <month>Jul</month> <day>28</day>,<volume>535</volume>(<issue>7613</issue>):<fpage>547</fpage>–<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature18635" xlink:type="simple">10.1038/nature18635</ext-link></comment> <object-id pub-id-type="pmid">27409816</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bailes</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Dean</surname> <given-names>RT</given-names></name>, <name name-style="western"><surname>Broughton</surname> <given-names>MC</given-names></name>. <article-title>How different are our perceptions of equal-tempered and microtonal intervals? A behavioural and EEG survey</article-title>. <source>PLoS ONE</source>. <year>2015</year> <month>Aug</month> <day>18</day>;<volume>10</volume>(<issue>8</issue>):<fpage>e0135082</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0135082" xlink:type="simple">10.1371/journal.pone.0135082</ext-link></comment> <object-id pub-id-type="pmid">26285010</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herff</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Olsen</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Dean</surname> <given-names>RT</given-names></name>, <name name-style="western"><surname>Prince</surname> <given-names>J</given-names></name>. <article-title>Memory for melodies in unfamiliar tuning systems: Investigating effects of recency and number of intervening items</article-title>. <source>Q J Exp Psychol</source>. <year>2017</year> <month>May</month> <day>26</day>,<volume>71</volume>:<fpage>1367</fpage>–<lpage>81</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leung</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Dean</surname> <given-names>RT</given-names></name>. <article-title>Learning unfamiliar pitch intervals: A novel paradigm for demonstrating the learning of statistical associations between musical pitches</article-title>. <source>PLoS ONE</source>. <year>2018</year>, <month>Aug</month> <day>30</day>,<volume>13</volume>(<issue>8</issue>):<fpage>e0203026</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0203026" xlink:type="simple">10.1371/journal.pone.0203026</ext-link></comment> <object-id pub-id-type="pmid">30161174</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leung</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Dean</surname> <given-names>RT</given-names></name>. <article-title>Learning a well-formed microtonal scale: Pitch intervals and event frequencies</article-title>. <source>Journal of New Music Research</source>. <year>2018</year> <month>Feb</month> <day>15</day>,<volume>8215</volume>:<fpage>1</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref024"><label>24</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Milne</surname> <given-names>AJ</given-names></name>. <source>A computational model of the cognition of tonality</source> [dissertation]. <publisher-loc>Milton Keynes (UK)</publisher-loc>: <publisher-name>The Open University</publisher-name>; <year>2013</year>.</mixed-citation></ref>
<ref id="pone.0218570.ref025"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">The Bohlen-Pierce site [Internet]. Huygens-Fokker; c2013 [cited 2019 Feb 5]. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.huygens-fokker.org/bpsite/" xlink:type="simple">http://www.huygens-fokker.org/bpsite/</ext-link></mixed-citation></ref>
<ref id="pone.0218570.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mathews</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>Pierce</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Reeves</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Roberts</surname> <given-names>L</given-names></name>. <article-title>Theoretical and experimental explorations of the Bohlen-Pierce scale</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>1988</year> <month>Jun</month>,<volume>84</volume>(<issue>4</issue>):<fpage>1214</fpage>–<lpage>1222</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bohlen</surname> <given-names>H</given-names></name>. <article-title>13 Tonstufen in der Duodezime</article-title>. <source>Acustica</source>, <month>Jan</month>;<volume>39</volume>:<fpage>67</fpage>–<lpage>86</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friedman</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Trammell Neill</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Seror</surname> <given-names>GA</given-names> <suffix>III</suffix></name>, <name name-style="western"><surname>Kleinsmith</surname> <given-names>AL</given-names></name>. <article-title>Average pitch height and perceived emotional expression within an unconventional tuning system</article-title>. <source>Music Perception</source> <year>2018</year> <month>Apr</month>,<volume>35</volume>(<issue>4</issue>):<fpage>518</fpage>–<lpage>523</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huron</surname> <given-names>D</given-names></name>. <article-title>A comparison of average pitch height and interval size in major- and minor-key themes: Evidence consistent with affect-related pitch prosody</article-title>. <source>Empirical Musicology Review</source>. <year>2008</year> <month>Apr</month>,<volume>3</volume>(<issue>2</issue>):<fpage>59</fpage>–<lpage>63</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huron</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>M</given-names></name>. <article-title>The harmonic minor scale provides an optimum way of reducing average melodic interval size, consistent with sad affect cues</article-title>. <source>Empirical Musicology Review</source>. <year>2012</year>,<volume>7</volume>(<issue>3</issue>):<fpage>103</fpage>–<lpage>117</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Temperley</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Tan</surname> <given-names>D</given-names></name>. <article-title>Emotional connotations of diatonic modes</article-title>. <source>Music Perception</source>. <year>2013</year>, <volume>30</volume>(<issue>3</issue>):<fpage>237</fpage>–<lpage>57</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loui</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Wessel</surname> <given-names>D L</given-names></name>, <name name-style="western"><surname>Hudson Kam</surname> <given-names>C L</given-names></name>. <article-title>Humans rapidly learn grammatical structure in a new musical scale</article-title>. <source>Music Perception</source>. <year>2010</year> <month>June</month> <day>1</day>,<volume>27</volume>(<issue>5</issue>):<fpage>377</fpage>–<lpage>388</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1525/mp.2010.27.5.377" xlink:type="simple">10.1525/mp.2010.27.5.377</ext-link></comment> <object-id pub-id-type="pmid">20740059</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loui</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>E H</given-names></name>, <name name-style="western"><surname>Wessel</surname> <given-names>D L</given-names></name>, <name name-style="western"><surname>Knight</surname> <given-names>R T</given-names></name>. <article-title>A generalized mechanism for perception of pitch patterns</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year> <month>Jan</month> <day>14</day>,<volume>29</volume>(<issue>2</issue>): <fpage>454</fpage>–<lpage>459</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4503-08.2009" xlink:type="simple">10.1523/JNEUROSCI.4503-08.2009</ext-link></comment> <object-id pub-id-type="pmid">19144845</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loui</surname> <given-names>P.</given-names></name> <article-title>Learning and liking of melody and harmony: Further studies in artificial grammar learning</article-title>. <source>Topics in Cognitive Science</source>. <year>2012</year> <month>Oct</month>,<volume>4</volume>(<issue>4</issue>):<fpage>554</fpage>–<lpage>567</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1756-8765.2012.01208.x" xlink:type="simple">10.1111/j.1756-8765.2012.01208.x</ext-link></comment> <object-id pub-id-type="pmid">22760940</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref035"><label>35</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Helmholtz</surname> <given-names>H</given-names></name>. <source>On the sensation of tones as a physiological basis for the theory of music</source>. <edition>2nd ed</edition>. <name name-style="western"><surname>Ellis</surname> <given-names>AJ</given-names></name>, editor. <publisher-loc>New York</publisher-loc>: <publisher-name>Dover</publisher-name>; <year>1954</year>/1877.</mixed-citation></ref>
<ref id="pone.0218570.ref036"><label>36</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Sethares</surname> <given-names>WA</given-names></name>. <source>Tuning, timbre, spectrum, scale</source>. <edition>2nd ed</edition>. <publisher-loc>London</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>2005</year>.</mixed-citation></ref>
<ref id="pone.0218570.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bowling</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Purves</surname> <given-names>D</given-names></name>. <article-title>A biological rationale for musical consonance</article-title>. <source>PNAS</source>. <year>2015</year> <month>Sep</month> <day>8</day>,<volume>112</volume>(<issue>36</issue>):<fpage>11155</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1505768112" xlink:type="simple">10.1073/pnas.1505768112</ext-link></comment> <object-id pub-id-type="pmid">26209651</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McLachlan</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Marco</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Light</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>S</given-names></name>. <article-title>Consonance and pitch</article-title>. <source>Journal of Experimental Psychology: General</source>. <year>2013</year> <month>Jan</month> <day>7</day>,<volume>142</volume>(<issue>4</issue>):<fpage>1142</fpage>–<lpage>1158</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson-Laird</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Kang</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Leon</surname> <given-names>Y.</given-names></name> <article-title>On musical dissonance</article-title>. <source>Music Perception</source>. <year>2012</year> <month>Sep</month>,<volume>30</volume>(<issue>1</issue>);<fpage>19</fpage>–<lpage>35</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dewitt</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Crowder</surname> <given-names>RG</given-names></name>. <article-title>Tonal fusion of consonant musical intervals: The oomph in Stumpf</article-title>. <source>Perception &amp; Psychophysics</source>. <year>1987</year> <month>Jan</month>,<volume>41</volume>(<issue>1</issue>):<fpage>73</fpage>–<lpage>84</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parncutt</surname> <given-names>R</given-names></name>. <article-title>The emotional connotations of major versus minor tonality: One or more origins?</article-title> <source>Musicae Scientiae</source>. <year>2014</year> <month>Aug</month> <day>20</day>,<volume>18</volume>(<issue>3</issue>):<fpage>324</fpage>–<lpage>353</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Milne</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Laney</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sharp</surname> <given-names>DB</given-names></name>. <article-title>Testing a spectral model of tonal affinity with microtonal melodies and inharmonic spectra</article-title>. <source>Musicae Scientiae</source>. <year>2016</year>, <volume>20</volume>(<issue>4</issue>):<fpage>465</fpage>–<lpage>494</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McDermott</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Lehr</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>. <article-title>Individual differences reveal the basis of consonance</article-title>. <source>Current Biology</source>. <year>2010</year> <month>Jun</month> <day>8</day>,<volume>20</volume>(<issue>11</issue>):<fpage>1035</fpage>–<lpage>1041</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2010.04.019" xlink:type="simple">10.1016/j.cub.2010.04.019</ext-link></comment> <object-id pub-id-type="pmid">20493704</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Milne</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Sethares</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Laney</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sharp</surname> <given-names>DB</given-names></name>. <article-title>Modelling the similarity of pitch collections with expectation tensors</article-title>. <source>Journal of Mathematics and Music</source>. <year>2011</year> <month>May</month>,<volume>5</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Terhardt</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Stoll</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Seewann</surname> <given-names>M</given-names></name>. <article-title>Algorithm for extraction of pitch and pitch salience from complex tonal signals</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>1982</year>, <volume>71</volume>:<fpage>679</fpage>–<lpage>88</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dubnov</surname> <given-names>S</given-names></name>. <article-title>Generalization of spectral flatness measure for non-gaussian linear processes</article-title>. <source>IEEE Signal Processing Letters</source>. <year>2004</year> <month>Jul</month> <day>26</day>,<volume>11</volume>(<issue>8</issue>):<fpage>698</fpage>–<lpage>701</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnston</surname> <given-names>J</given-names></name>. <article-title>Transform coding of audio signals using perceptual noise criteria</article-title>. <source>IEEE Journal on Selected Areas in Communications</source>. <year>1988</year> <month>Feb</month>, <volume>6</volume>(<issue>2</issue>):<fpage>314</fpage>–<lpage>332</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gingras</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Marin</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Fitch</surname> <given-names>WT</given-names></name>. <article-title>Beyond intensity: Spectral features effectively predict music-induced subjective arousal</article-title>. <source>Q J Exp Psychol</source>. <year>2014</year>;<volume>67</volume>(<issue>7</issue>):<fpage>1428</fpage>–<lpage>46</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Milne</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Bulger</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Herff</surname> <given-names>SA</given-names></name>. <article-title>Exploring the space of perfectly balanced rhythms and scales</article-title>. <source>Journal of Mathematics and Music</source>. <year>2017</year>, <volume>11</volume>(<issue>2–3</issue>):<fpage>101</fpage>–<lpage>33</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Milne</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Holland</surname> <given-names>S</given-names></name>. <article-title>Empirically testing Tonnetz, voice-leading, and spectral models of perceived triadic distance</article-title>. <source>Journal of Mathematics and Music</source>. <year>2016</year> <month>Apr</month> <day>26</day>,<volume>10</volume>(<issue>1</issue>):<fpage>59</fpage>–<lpage>85</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hong</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Chau</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Horner</surname> <given-names>A</given-names></name>. <article-title>How often and why mode fails to predict mode in low-arousal classical piano music</article-title>. <source>Journal of New Music Research</source>. <year>2018</year> <month>Jun</month> <day>20</day>,<volume>47</volume>(<issue>5</issue>):<fpage>462</fpage>–<lpage>75</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Oshinsky</surname> <given-names>JS</given-names></name>. <article-title>Cue utilization in emotion attribution from auditory stimuli</article-title>. <source>Motivation and Emotion</source>. <year>1977</year> <month>Dec</month>,<volume>1</volume>(<issue>4</issue>):<fpage>331</fpage>–<lpage>46</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maher</surname> <given-names>T</given-names></name>. <article-title>A rigorous test of the proposition that musical intervals have different psychological effects</article-title>. <source>Am J Psychol</source>. <year>1980</year> <month>Jun</month>,<volume>93</volume><italic>(</italic><issue>2</issue>):<fpage>309</fpage>–<lpage>27</lpage>. <object-id pub-id-type="pmid">7406071</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref054"><label>54</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Lahdelma</surname> <given-names>I</given-names></name>. <source>At the interface between sensation and emotion: perceived qualities of single chords</source> [dissertation]. <publisher-loc>Jyväskylä, Finland</publisher-loc>: <publisher-name>University of Jyväskylä</publisher-name>; <year>2017</year>.</mixed-citation></ref>
<ref id="pone.0218570.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parncutt</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hair</surname> <given-names>G</given-names></name>. <article-title>Consonance and dissonance in music theory and psychology: Disentangling dissonant dichotomies</article-title>. <source>Journal of Interdisciplinary Music Studies</source>. <year>2011</year>, <volume>5</volume>(<issue>2</issue>):<fpage>119</fpage>–<lpage>166</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Müllensiefen</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Gingras</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Musil</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Stewart</surname> <given-names>L</given-names></name>. <article-title>The musicality of non-musicians: An index for assessing musical sophistication in the general population</article-title>. <source>PLoS ONE</source>. <year>2014</year> <month>Feb</month> <day>26</day>,<volume>9</volume>(<issue>2</issue>):<fpage>e101091</fpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Susino</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McPherson</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Schubert</surname> <given-names>E</given-names></name>. <article-title>The definition of a musician in music psychology: A literature review and the six-year rule</article-title>. <source>Psychology of Music</source>. <year>2018</year>, <month>Oct</month> <day>22</day>: <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0305735618804038" xlink:type="simple">10.1177/0305735618804038</ext-link></comment></mixed-citation></ref>
<ref id="pone.0218570.ref058"><label>58</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>De Finetti</surname> <given-names>B</given-names></name>. <source>Theory of probability</source>. Vol <volume>1</volume>. <publisher-loc>New York</publisher-loc>: <publisher-name>John Wiley and Sons</publisher-name>; <year>1974</year>.</mixed-citation></ref>
<ref id="pone.0218570.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van de Schoot</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Depaoli</surname> <given-names>S</given-names></name>. <article-title>Bayesian analyses: where to start and what to report</article-title>. <source>The European Health Psychologist</source>. <year>2014</year>, <volume>16</volume>(<issue>2</issue>):<fpage>75</fpage>–<lpage>84</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref060"><label>60</label><mixed-citation publication-type="book" xlink:type="simple"><collab>R Core Team</collab>. <source>R: A language and environment for statistical computing</source>. <publisher-name>R Foundation for Statistical Computing</publisher-name>, <publisher-loc>Vienna, Austria</publisher-loc>; <year>2014</year>.</mixed-citation></ref>
<ref id="pone.0218570.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bürkner</surname> <given-names>P-C</given-names></name>. <article-title>brms: An R package for Bayesian multilevel models using Stan</article-title>. <source>Journal of Statistical Software</source>. <year>2017</year> <month>Aug</month> <day>29</day>,<volume>80</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>28</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bürkner</surname> <given-names>P-C</given-names></name>. <article-title>Advanced Bayesian multilevel modelling with the R package brms</article-title>. <source>The R Journal</source>. <year>2018</year> <month>Jul</month>,<volume>10</volume>(<issue>1</issue>):<fpage>395</fpage>–<lpage>411</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barr</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Levy</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Scheepers</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tilyc</surname> <given-names>HJ</given-names></name>. <article-title>Random effects structure for confirmatory hypothesis testing: Keep it maximal</article-title>. <source>Journal of Memory and Language</source>. <year>2013</year> <month>Apr</month>, <volume>68</volume>(<issue>3</issue>):<fpage>255</fpage>–<lpage>278</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref064"><label>64</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kruschke</surname> <given-names>JK</given-names></name>. <source>Doing Bayesian Data analysis</source>. <edition>2nd ed</edition>. <publisher-loc>Burlington</publisher-loc>: <publisher-name>Academic Press/Elsevier</publisher-name>; <year>2015</year>.</mixed-citation></ref>
<ref id="pone.0218570.ref065"><label>65</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Jeffreys</surname> <given-names>H</given-names></name>. <source>The theory of probability</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>1961</year>.</mixed-citation></ref>
<ref id="pone.0218570.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kruschke</surname> <given-names>JK</given-names></name>. <article-title>Rejecting or accepting parameter values in Bayesian estimation</article-title>. <source>Advances in Methods and Practices in Psychological Science</source>. <year>2018</year>, <month>May</month> <day>8</day>,<volume>1</volume>(<issue>2</issue>):<fpage>270</fpage>–<lpage>280</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hwang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Vehtari</surname> <given-names>A</given-names></name>. <article-title>Understanding predictive information criteria for Bayesian models</article-title>. <source>Statistics and Computing</source>. <year>2014</year> <month>Nov</month>,<volume>24</volume>(<issue>6</issue>):<fpage>997</fpage>–<lpage>1016</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Guo</surname> <given-names>J</given-names></name>. <article-title>Stan: A probabilistic programming language for Bayesian inference and optimization</article-title>. <source>Journal of Educational and Behavioral Statistics</source>. <year>2015</year> <month>Oct</month> <day>1</day>,<volume>40</volume>(<issue>5</issue>):<fpage>530</fpage>–<lpage>543</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stolzenburg</surname> <given-names>F</given-names></name>. <article-title>Harmony perception by periodicity detection</article-title>. <source>Journal of Mathematics and Music</source>. <year>2015</year>. <volume>9</volume>(<issue>3</issue>):<fpage>215</fpage>–<lpage>38</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Terhardt</surname> <given-names>E</given-names></name>. <article-title>Pitch, consonance, and harmony</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1974</year>, <volume>55</volume>(<issue>5</issue>):<fpage>1061</fpage>–<lpage>69</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1914648" xlink:type="simple">10.1121/1.1914648</ext-link></comment> <object-id pub-id-type="pmid">4833699</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van De Geer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Levelt</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Plomp</surname> <given-names>R</given-names></name>. <article-title>The connotation of musical consonance.</article-title> <source>Acta Psychologica</source>. <year>1962</year>, <volume>20</volume>:<fpage>308</fpage>–<lpage>319</lpage>.</mixed-citation></ref>
<ref id="pone.0218570.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>. <article-title>Expression of emotion in voice and music</article-title>. <source>Journal of Voice</source>. <year>1995</year> <month>Sep</month>,<volume>9</volume>(<issue>3</issue>):<fpage>235</fpage>–<lpage>48</lpage>. <object-id pub-id-type="pmid">8541967</object-id></mixed-citation></ref>
<ref id="pone.0218570.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bowling</surname> <given-names>DL</given-names></name>. <article-title>A vocal basis for the affective character of musical mode in melody</article-title>. <source>Front Psychol</source>. <year>2013</year> <month>Jul</month> <day>31</day>,<volume>4</volume>:<fpage>464</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2013.00464" xlink:type="simple">10.3389/fpsyg.2013.00464</ext-link></comment> <object-id pub-id-type="pmid">23914179</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>